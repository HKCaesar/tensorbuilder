# Tensor Builder

TensorBuilder is light wrapper over TensorFlow that enables you to easily create complex deep neural networks using the Builder Pattern through a functional [fluent](https://en.wikipedia.org/wiki/Fluent_interface) [immutable](https://en.wikipedia.org/wiki/Immutable_object) API. The main goals of TensorBuilder are

* Be a light-weight wrapper around TensorFlow fully compatible with its functions.
* Let the user name and inspect the variables tensorflow Variables generated by TensorBuilder
* Enable the user to create complex branched topologies (see the Builder.branch method on the docs)

One of the things that makes TensorBuilder very nice to work with when building complex neural networks is that the branching mechanism enable you to express through the structure of your code the structure of the network, even when you have complex sub-branching expansions and reductions, all this while keeping the same fluid API.

TensorBuilder takes inspiration from [prettytensor](https://github.com/google/prettytensor) but its internals are simpler, its API is smaller but equally powerfull, and its branching mechanism is more expresive maintains the fluent API. The point where they really diverge is that PrettyTensor uses the `with` statement to branch the computation, this has several drawback:

* Breaks the fluent API since you cant chain `with` statements
* Each branch is really an object with mutable state instead of a (real) branch in the code
* It has implicit behavior: it appear that the that when the `with` statemente closes it forcefully reduces the branches to a single layer.

On the other hand, TensorBuilder has a small set of primitives (less 15 methods of which you mostly use 6) that enable you to express complex networks while maintaining a consistent API.

## Installation

At the moment the easiest way to install it in your project is to do the following

1. `cd` to your project
2. `git clone https://github.com/cgarciae/tensorbuilder.git`
3. Erase the .git file/folder`rm tensorbuilder/.git` or `rm -fr tensorbuilder/.git`

## Getting Started

Create neural network with a [5, 10, 3] architecture with a `softmax` output layer and a `tanh` hidden layer through a Builder and then get back its tensor:

    import tensorflow as tf
    import tensorbuilder as tb

    x = tf.placeholder(tf.float32, shape=[None, 5])

    h = (
        x.builder()
        .connect_layer(10, fn=tf.nn.tanh)
        .connect_layer(3, fn=tf.nn.softmax)
        .tensor
    )

## Branching
If you are sufficiently familiar with tensorflow or use prettytensor then you might appreciate the branching capabilities of Tensor Builder in this (overly complex) example

    import tensorflow as tf
    import tensorbuilder as tb

    x = tf.placeholder(tf.float32, shape=[None, 5])
    keep_prob = tf.placeholder(tf.float32)

    h = (
        x.builder()
        .connect_layer(10)
        .branch(lambda root:
        [
            root
            .connect_layer(3, fn=tf.nn.relu)
        ,
            root
            .connect_layer(9, fn=tf.nn.tanh)
            .branch(lambda root2: 
            [
              root2
              .connect_layer(6, fn=tf.nn.sigmoid)
            ,
              root2
              .map(tf.nn.dropout, keep_prob)
              .connect_layer(8, tf.nn.softmax)
            ])
        ])
        .connect_layer(6, fn=tf.nn.sigmoid)
        .tensor
    )

## Documentation

The main documentaion is in the [tensorbuilder module](http://cgarciae.github.io/tensorbuilder/tensorbuilder.m.html). The documentation for the complete project is [here](http://cgarciae.github.io/tensorbuilder/).

## Examples
##############################
##### FUNCTIONS
##############################


##############################
##### builder
##############################

The following example shows you how to construct a `tensorbuilder.tensorbuilder.Builder` from a tensorflow Tensor.

    import tensorflow as tf
    import tensorbuilder as tb

    a = tf.placeholder(tf.float32, shape=[None, 8])
    a_builder = tb.builder(a)

The previous is the same as

    a_builder = tf.placeholder(tf.float32, shape=[None, 8]).builder()

since tensorbuilder monkey-patches tensorflow's Tensor with this function as method.

##############################
##### branches
##############################

Given a list of Builders and/or BuilderTrees you construct a `tensorbuilder.tensorbuilder.BuilderTree`.

    import tensorflow as tf
    import tensorbuilder as tb

    a = tf.placeholder(tf.float32, shape=[None, 8]).builder()
    b = tf.placeholder(tf.float32, shape=[None, 8]).builder()

    tree = tb.branches([a, b])

`tensorbuilder.tensorbuilder.BuilderTree`s are usually constructed using `tensorbuilder.tensorbuilder.Builder.branch` of the `tensorbuilder.tensorbuilder.Builder` class, but you can use this for special cases



##############################
##### BUILDER
##############################

##############################
##### connect_weights
##############################

The following builds `tf.matmul(x, w)`

    import tensorflow as tf
    import tensorbuilder as tb

    x = tf.placeholder(tf.float32, shape=[None, 5])

    z = x.builder().connect_weights(3, weights_name="weights") 



##############################
##### connect_bias
##############################

The following builds `tf.matmul(x, w) + b`

    import tensorflow as tf
    import tensorbuilder as tb

    x = tf.placeholder(tf.float32, shape=[None, 5])

    z = (
      x.builder()
      .connect_weights(3, weights_name="weights")
      .connect_bias(bias_name="bias")
    )

Note, the previous is equivalent to using `tensorbuilder.tensorbuilder.Builder.connect_layer` like this

    z = (
      x.builder()
      .connect_layer(3, weights_name="weights", bias_name="bias")
    )




##############################
##### connect_layer
##############################

The following builds the computation `tf.nn.sigmoid(tf.matmul(x, w) + b)`

    import tensorflow as tf
    import tensorbuilder as tb

    x = tf.placeholder(tf.float32, shape=[None, 5])

    h = (
      x.builder()
      .connect_layer(3, fn=tf.nn.sigmoid, weights_name="weights", bias_name="bias")
    )

The previous is equivalent to using 

    h = (
      x.builder()
      .connect_weights(3, weights_name="weights")
      .connect_bias(bias_name="bias")
      .map(tf.nn.sigmoid)
)

You can chain various `connect_layer`s to get deeper neural networks

    import tensorflow as tf
    import tensorbuilder as tb

    x = tf.placeholder(tf.float32, shape=[None, 40])

    h = (
      x.builder()
      .connect_layer(100, fn=tf.nn.tanh)
      .connect_layer(30, fn=tf.nn.softmax)
    )




##############################
##### map
##############################

The following constructs a neural network with the architecture `[40 input, 100 tanh, 30 softmax]` and and applies `dropout` to the tanh layer

    import tensorflow as tf
    import tensorbuilder as tb

    x = tf.placeholder(tf.float32, shape=[None, 40])
    keep_prob = tf.placeholder(tf.float32)

    h = (
      x.builder()
      .connect_layer(100, fn=tf.nn.tanh)
      .map(tf.nn.dropout, keep_prob)
      .connect_layer(30, fn=tf.nn.softmax)
    )




##############################
##### then
##############################

The following *manually* constructs the computation `tf.nn.sigmoid(tf.matmul(x, w) + b)` while updating the `tensorbuilder.tensorbuiler.Builder.variables` dictionary.

    import tensorflow as tf
    import tensorbuilder as tb

    x = tf.placeholder(tf.float32, shape=[None, 40])
    keep_prob = tf.placeholder(tf.float32)

    def sigmoid_layer(builder, size):
      m = int(builder.tensor.get_shape()[1])
      n = size

      w = tf.Variable(tf.random_uniform([m, n], -1.0, 1.0))
      b = tf.Variable(tf.random_uniform([n], -1.0, 1.0))

      builder.variables[w.name] = w
      builder.variables[b.name] = b

      builder.tensor = tf.nn.sigmoid(tf.matmul(builder.tensor, w) + b)

      return builder

    h = (
      x.builder()
      .then(lambda builder: sigmoid_layer(builder, 3))
    )

Note that the previous if equivalent to

    h = (
      x.builder()
      .connect_layer(3, fn=tf.nn.sigmoid)
    )




##############################
##### branch
##############################

The following will create a sigmoid layer but will branch the computation at the logit (z) so you get both the output tensor `h` and `trainer` tensor. Observe that first the logit `z` is calculated by creating a linear layer with `connect_layer(1)` and then its branched out

    import tensorflow as tf
    import tensorbuilder as tb

    x = tf.placeholder(tf.float32, shape=[None, 5])
    y = tf.placeholder(tf.float32, shape=[None, 1])

    [h, trainer] = (
        x.builder()
        .connect_layer(1)
        .branch(lambda z:
        [
            z.map(tf.nn.sigmoid)
        ,
            z.map(tf.nn.sigmoid_cross_entropy_with_logits, y)
            .map(tf.train.AdamOptimizer(0.01).minimize)
        ])
        .tensors()
    )

Note that you have to use the `tensorbuilder.tensorbuilder.BuilderTree.tensors` method from the `tensorbuilder.tensorbuilder.BuilderTree` class to get the tensors back. Remember that you can also contain `tensorbuilder.tensorbuilder.BuilderTree` elements when you branch out, this means that you can keep branching inside branch. Don't worry that the tree keep getting deeper, `tensorbuilder.tensorbuilder.BuilderTree` has methods that help you flatten or reduce the tree. 

The following example will show you how create a (overly) complex tree and then connect all the leaf nodes to a single `sigmoid` layer

    import tensorflow as tf
    import tensorbuilder as tb

    x = tf.placeholder(tf.float32, shape=[None, 5])
    keep_prob = tf.placeholder(tf.float32)

    h = (
        x.builder()
        .connect_layer(10)
        .branch(lambda base:
        [
            base
            .connect_layer(3, fn=tf.nn.relu)
        ,
            base
            .connect_layer(9, fn=tf.nn.tanh)
            .branch(lambda base2: 
            [
              base2
              .connect_layer(6, fn=tf.nn.sigmoid)
            ,
              base2
              .map(tf.nn.dropout, keep_prob)
              .connect_layer(8, tf.nn.softmax)
            ])
        ])
        .connect_layer(6, fn=tf.nn.sigmoid)
    )

##############################
##### BUILDER TREE
##############################

##############################
##### builder
##############################

    import tensorflow as tf
    import tensorbuilder as tb

    x = tf.placeholder(tf.float32, shape=[None, 5])
    y = tf.placeholder(tf.float32, shape=[None, 1])

    [h_builder, trainer_builder] = (
        x.builder()
        .connect_layer(1)
        .branch(lambda z:
        [
            z.map(tf.nn.sigmoid)
        ,
            z.map(tf.nn.sigmoid_cross_entropy_with_logits, y)
            .map(tf.train.AdamOptimizer(0.01).minimize)
        ])
        .builders()
    )



##############################
##### tensors
##############################

    import tensorflow as tf
    import tensorbuilder as tb

    x = tf.placeholder(tf.float32, shape=[None, 5])
    y = tf.placeholder(tf.float32, shape=[None, 1])

    [h_tensor, trainer_tensor] = (
        x.builder()
        .connect_layer(1)
        .branch(lambda z:
        [
            z.map(tf.nn.sigmoid)
        ,
            z.map(tf.nn.sigmoid_cross_entropy_with_logits, y)
            .map(tf.train.AdamOptimizer(0.01).minimize)
        ])
        .tensors()
    )

##############################
##### connect_layer
##############################

The following example shows you how to connect two tensors (rather builders) of different shapes to a single `softmax` layer of shape [None, 3]

    import tensorflow as tf
    import tensorbuilder as tb

    a = tf.placeholder(tf.float32, shape=[None, 8]).builder()
    b = tf.placeholder(tf.float32, shape=[None, 5]).builder()

    h = (
      tb.branches([a, b])
      .connect_layer(3, fn=tf.nn.softmax)
    )

The next example show you how you can use this to pass the input layer directly through one branch, and "analyze" it with a `tanh layer` filter through the other, both of these are connect to a single `softmax` output layer

    import tensorflow as tf
    import tensorbuilder as tb

    x = tf.placeholder(tf.float32, shape=[None, 5])

    h = (
      x.builder()
      .branch(lambda x: 
      [
        x
      ,
        x.connect_layer(10, fn=tf.nn.tanh)
      ])
      .connect_layer(3, fn=tf.nn.softmax)
    )