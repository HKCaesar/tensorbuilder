<!doctype html>
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />

    <title>tensorbuilder.core API documentation</title>
    <meta name="description" content="" />

  <link href='http://fonts.googleapis.com/css?family=Source+Sans+Pro:400,300' rel='stylesheet' type='text/css'>
  
  <style type="text/css">
  
* {
  box-sizing: border-box;
}
/*! normalize.css v1.1.1 | MIT License | git.io/normalize */

/* ==========================================================================
   HTML5 display definitions
   ========================================================================== */

/**
 * Correct `block` display not defined in IE 6/7/8/9 and Firefox 3.
 */

article,
aside,
details,
figcaption,
figure,
footer,
header,
hgroup,
main,
nav,
section,
summary {
    display: block;
}

/**
 * Correct `inline-block` display not defined in IE 6/7/8/9 and Firefox 3.
 */

audio,
canvas,
video {
    display: inline-block;
    *display: inline;
    *zoom: 1;
}

/**
 * Prevent modern browsers from displaying `audio` without controls.
 * Remove excess height in iOS 5 devices.
 */

audio:not([controls]) {
    display: none;
    height: 0;
}

/**
 * Address styling not present in IE 7/8/9, Firefox 3, and Safari 4.
 * Known issue: no IE 6 support.
 */

[hidden] {
    display: none;
}

/* ==========================================================================
   Base
   ========================================================================== */

/**
 * 1. Prevent system color scheme's background color being used in Firefox, IE,
 *    and Opera.
 * 2. Prevent system color scheme's text color being used in Firefox, IE, and
 *    Opera.
 * 3. Correct text resizing oddly in IE 6/7 when body `font-size` is set using
 *    `em` units.
 * 4. Prevent iOS text size adjust after orientation change, without disabling
 *    user zoom.
 */

html {
    background: #fff; /* 1 */
    color: #000; /* 2 */
    font-size: 100%; /* 3 */
    -webkit-text-size-adjust: 100%; /* 4 */
    -ms-text-size-adjust: 100%; /* 4 */
}

/**
 * Address `font-family` inconsistency between `textarea` and other form
 * elements.
 */

html,
button,
input,
select,
textarea {
    font-family: sans-serif;
}

/**
 * Address margins handled incorrectly in IE 6/7.
 */

body {
    margin: 0;
}

/* ==========================================================================
   Links
   ========================================================================== */

/**
 * Address `outline` inconsistency between Chrome and other browsers.
 */

a:focus {
    outline: thin dotted;
}

/**
 * Improve readability when focused and also mouse hovered in all browsers.
 */

a:active,
a:hover {
    outline: 0;
}

/* ==========================================================================
   Typography
   ========================================================================== */

/**
 * Address font sizes and margins set differently in IE 6/7.
 * Address font sizes within `section` and `article` in Firefox 4+, Safari 5,
 * and Chrome.
 */

h1 {
    font-size: 2em;
    margin: 0.67em 0;
}

h2 {
    font-size: 1.5em;
    margin: 0.83em 0;
}

h3 {
    font-size: 1.17em;
    margin: 1em 0;
}

h4 {
    font-size: 1em;
    margin: 1.33em 0;
}

h5 {
    font-size: 0.83em;
    margin: 1.67em 0;
}

h6 {
    font-size: 0.67em;
    margin: 2.33em 0;
}

/**
 * Address styling not present in IE 7/8/9, Safari 5, and Chrome.
 */

abbr[title] {
    border-bottom: 1px dotted;
}

/**
 * Address style set to `bolder` in Firefox 3+, Safari 4/5, and Chrome.
 */

b,
strong {
    font-weight: bold;
}

blockquote {
    margin: 1em 40px;
}

/**
 * Address styling not present in Safari 5 and Chrome.
 */

dfn {
    font-style: italic;
}

/**
 * Address differences between Firefox and other browsers.
 * Known issue: no IE 6/7 normalization.
 */

hr {
    -moz-box-sizing: content-box;
    box-sizing: content-box;
    height: 0;
}

/**
 * Address styling not present in IE 6/7/8/9.
 */

mark {
    background: #ff0;
    color: #000;
}

/**
 * Address margins set differently in IE 6/7.
 */

p,
pre {
    margin: 1em 0;
}

/**
 * Correct font family set oddly in IE 6, Safari 4/5, and Chrome.
 */

code,
kbd,
pre,
samp {
    font-family: monospace, serif;
    _font-family: 'courier new', monospace;
    font-size: 1em;
}

/**
 * Improve readability of pre-formatted text in all browsers.
 */

pre {
    white-space: pre;
    white-space: pre-wrap;
    word-wrap: break-word;
}

/**
 * Address CSS quotes not supported in IE 6/7.
 */

q {
    quotes: none;
}

/**
 * Address `quotes` property not supported in Safari 4.
 */

q:before,
q:after {
    content: '';
    content: none;
}

/**
 * Address inconsistent and variable font size in all browsers.
 */

small {
    font-size: 80%;
}

/**
 * Prevent `sub` and `sup` affecting `line-height` in all browsers.
 */

sub,
sup {
    font-size: 75%;
    line-height: 0;
    position: relative;
    vertical-align: baseline;
}

sup {
    top: -0.5em;
}

sub {
    bottom: -0.25em;
}

/* ==========================================================================
   Lists
   ========================================================================== */

/**
 * Address margins set differently in IE 6/7.
 */

dl,
menu,
ol,
ul {
    margin: 1em 0;
}

dd {
    margin: 0 0 0 40px;
}

/**
 * Address paddings set differently in IE 6/7.
 */

menu,
ol,
ul {
    padding: 0 0 0 40px;
}

/**
 * Correct list images handled incorrectly in IE 7.
 */

nav ul,
nav ol {
    list-style: none;
    list-style-image: none;
}

/* ==========================================================================
   Embedded content
   ========================================================================== */

/**
 * 1. Remove border when inside `a` element in IE 6/7/8/9 and Firefox 3.
 * 2. Improve image quality when scaled in IE 7.
 */

img {
    border: 0; /* 1 */
    -ms-interpolation-mode: bicubic; /* 2 */
}

/**
 * Correct overflow displayed oddly in IE 9.
 */

svg:not(:root) {
    overflow: hidden;
}

/* ==========================================================================
   Figures
   ========================================================================== */

/**
 * Address margin not present in IE 6/7/8/9, Safari 5, and Opera 11.
 */

figure {
    margin: 0;
}

/* ==========================================================================
   Forms
   ========================================================================== */

/**
 * Correct margin displayed oddly in IE 6/7.
 */

form {
    margin: 0;
}

/**
 * Define consistent border, margin, and padding.
 */

fieldset {
    border: 1px solid #c0c0c0;
    margin: 0 2px;
    padding: 0.35em 0.625em 0.75em;
}

/**
 * 1. Correct color not being inherited in IE 6/7/8/9.
 * 2. Correct text not wrapping in Firefox 3.
 * 3. Correct alignment displayed oddly in IE 6/7.
 */

legend {
    border: 0; /* 1 */
    padding: 0;
    white-space: normal; /* 2 */
    *margin-left: -7px; /* 3 */
}

/**
 * 1. Correct font size not being inherited in all browsers.
 * 2. Address margins set differently in IE 6/7, Firefox 3+, Safari 5,
 *    and Chrome.
 * 3. Improve appearance and consistency in all browsers.
 */

button,
input,
select,
textarea {
    font-size: 100%; /* 1 */
    margin: 0; /* 2 */
    vertical-align: baseline; /* 3 */
    *vertical-align: middle; /* 3 */
}

/**
 * Address Firefox 3+ setting `line-height` on `input` using `!important` in
 * the UA stylesheet.
 */

button,
input {
    line-height: normal;
}

/**
 * Address inconsistent `text-transform` inheritance for `button` and `select`.
 * All other form control elements do not inherit `text-transform` values.
 * Correct `button` style inheritance in Chrome, Safari 5+, and IE 6+.
 * Correct `select` style inheritance in Firefox 4+ and Opera.
 */

button,
select {
    text-transform: none;
}

/**
 * 1. Avoid the WebKit bug in Android 4.0.* where (2) destroys native `audio`
 *    and `video` controls.
 * 2. Correct inability to style clickable `input` types in iOS.
 * 3. Improve usability and consistency of cursor style between image-type
 *    `input` and others.
 * 4. Remove inner spacing in IE 7 without affecting normal text inputs.
 *    Known issue: inner spacing remains in IE 6.
 */

button,
html input[type="button"], /* 1 */
input[type="reset"],
input[type="submit"] {
    -webkit-appearance: button; /* 2 */
    cursor: pointer; /* 3 */
    *overflow: visible;  /* 4 */
}

/**
 * Re-set default cursor for disabled elements.
 */

button[disabled],
html input[disabled] {
    cursor: default;
}

/**
 * 1. Address box sizing set to content-box in IE 8/9.
 * 2. Remove excess padding in IE 8/9.
 * 3. Remove excess padding in IE 7.
 *    Known issue: excess padding remains in IE 6.
 */

input[type="checkbox"],
input[type="radio"] {
    box-sizing: border-box; /* 1 */
    padding: 0; /* 2 */
    *height: 13px; /* 3 */
    *width: 13px; /* 3 */
}

/**
 * 1. Address `appearance` set to `searchfield` in Safari 5 and Chrome.
 * 2. Address `box-sizing` set to `border-box` in Safari 5 and Chrome
 *    (include `-moz` to future-proof).
 */

input[type="search"] {
    -webkit-appearance: textfield; /* 1 */
    -moz-box-sizing: content-box;
    -webkit-box-sizing: content-box; /* 2 */
    box-sizing: content-box;
}

/**
 * Remove inner padding and search cancel button in Safari 5 and Chrome
 * on OS X.
 */

input[type="search"]::-webkit-search-cancel-button,
input[type="search"]::-webkit-search-decoration {
    -webkit-appearance: none;
}

/**
 * Remove inner padding and border in Firefox 3+.
 */

button::-moz-focus-inner,
input::-moz-focus-inner {
    border: 0;
    padding: 0;
}

/**
 * 1. Remove default vertical scrollbar in IE 6/7/8/9.
 * 2. Improve readability and alignment in all browsers.
 */

textarea {
    overflow: auto; /* 1 */
    vertical-align: top; /* 2 */
}

/* ==========================================================================
   Tables
   ========================================================================== */

/**
 * Remove most spacing between table cells.
 */

table {
    border-collapse: collapse;
    border-spacing: 0;
}

  </style>

  <style type="text/css">
  
  html, body {
    margin: 0;
    padding: 0;
    min-height: 100%;
  }
  body {
    background: #fff;
    font-family: "Source Sans Pro", "Helvetica Neueue", Helvetica, sans;
    font-weight: 300;
    font-size: 16px;
    line-height: 1.6em;
  }
  #content {
    width: 70%;
    max-width: 850px;
    float: left;
    padding: 30px 60px;
    border-left: 1px solid #ddd;
  }
  #sidebar {
    width: 25%;
    float: left;
    padding: 30px;
    overflow: hidden;
  }
  #nav {
    font-size: 130%;
    margin: 0 0 15px 0;
  }

  #top {
    display: block;
    position: fixed;
    bottom: 5px;
    left: 5px;
    font-size: .85em;
    text-transform: uppercase;
  }

  #footer {
    font-size: .75em;
    padding: 5px 30px;
    border-top: 1px solid #ddd;
    text-align: right;
  }
    #footer p {
      margin: 0 0 0 30px;
      display: inline-block;
    }

  h1, h2, h3, h4, h5 {
    font-weight: 300;
  }
  h1 {
    font-size: 2.5em;
    line-height: 1.1em;
    margin: 0 0 .50em 0;
  }

  h2 {
    font-size: 1.75em;
    margin: 1em 0 .50em 0;
  }

  h3 {
    margin: 25px 0 10px 0;
  }

  h4 {
    margin: 0;
    font-size: 105%;
  }

  a {
    color: #058;
    text-decoration: none;
    transition: color .3s ease-in-out;
  }

  a:hover {
    color: #e08524;
    transition: color .3s ease-in-out;
  }

  pre, code, .mono, .name {
    font-family: "Ubuntu Mono", "Cousine", "DejaVu Sans Mono", monospace;
  }

  .title .name {
    font-weight: bold;
  }
  .section-title {
    margin-top: 2em;
  }
  .ident {
    color: #900;
  }

  code {
    background: #f9f9f9;
  } 

  pre {
    background: #fefefe;
    border: 1px solid #ddd;
    box-shadow: 2px 2px 0 #f3f3f3;
    margin: 0 30px;
    padding: 15px 30px;
  }

  .codehilite {
    margin: 0 30px 10px 30px;
  }

    .codehilite pre {
      margin: 0;
    }
    .codehilite .err { background: #ff3300; color: #fff !important; } 

  table#module-list {
    font-size: 110%;
  }

    table#module-list tr td:first-child {
      padding-right: 10px;
      white-space: nowrap;
    }

    table#module-list td {
      vertical-align: top;
      padding-bottom: 8px;
    }

      table#module-list td p {
        margin: 0 0 7px 0;
      }

  .def {
    display: table;
  }

    .def p {
      display: table-cell;
      vertical-align: top;
      text-align: left;
    }

    .def p:first-child {
      white-space: nowrap;
    }

    .def p:last-child {
      width: 100%;
    }


  #index {
    list-style-type: none;
    margin: 0;
    padding: 0;
  }
    ul#index .class_name {
      /* font-size: 110%; */
      font-weight: bold;
    }
    #index ul {
      margin: 0;
    }

  .item {
    margin: 0 0 15px 0;
  }

    .item .class {
      margin: 0 0 25px 30px;
    }

      .item .class ul.class_list {
        margin: 0 0 20px 0;
      }

    .item .name {
      background: #fafafa;
      margin: 0;
      font-weight: bold;
      padding: 5px 10px;
      border-radius: 3px;
      display: inline-block;
      min-width: 40%;
    }
      .item .name:hover {
        background: #f6f6f6;
      }

    .item .empty_desc {
      margin: 0 0 5px 0;
      padding: 0;
    }

    .item .inheritance {
      margin: 3px 0 0 30px;
    }

    .item .inherited {
      color: #666;
    }

    .item .desc {
      padding: 0 8px;
      margin: 0;
    }

      .item .desc p {
        margin: 0 0 10px 0;
      }

    .source_cont {
      margin: 0;
      padding: 0;
    }

    .source_link a {
      background: #ffc300;
      font-weight: 400;
      font-size: .75em;
      text-transform: uppercase;
      color: #fff;
      text-shadow: 1px 1px 0 #f4b700;
      
      padding: 3px 8px;
      border-radius: 2px;
      transition: background .3s ease-in-out;
    }
      .source_link a:hover {
        background: #FF7200;
        text-shadow: none;
        transition: background .3s ease-in-out;
      }

    .source {
      display: none;
      max-height: 600px;
      overflow-y: scroll;
      margin-bottom: 15px;
    }

      .source .codehilite {
        margin: 0;
      }

  .desc h1, .desc h2, .desc h3 {
    font-size: 100% !important;
  }
  .clear {
    clear: both;
  }

  @media all and (max-width: 950px) {
    #sidebar {
      width: 35%;
    }
    #content {
      width: 65%;
    }
  }
  @media all and (max-width: 650px) {
    #top {
      display: none;
    }
    #sidebar {
      float: none;
      width: auto;
    }
    #content {
      float: none;
      width: auto;
      padding: 30px;
    }

    #index ul {
      padding: 0;
      margin-bottom: 15px;
    }
    #index ul li {
      display: inline-block;
      margin-right: 30px;
    }
    #footer {
      text-align: left;
    }
    #footer p {
      display: block;
      margin: inherit;
    }
  }

  /*****************************/

  </style>

  <style type="text/css">
  .codehilite .hll { background-color: #ffffcc }
.codehilite  { background: #f8f8f8; }
.codehilite .c { color: #408080; font-style: italic } /* Comment */
.codehilite .err { border: 1px solid #FF0000 } /* Error */
.codehilite .k { color: #008000; font-weight: bold } /* Keyword */
.codehilite .o { color: #666666 } /* Operator */
.codehilite .ch { color: #408080; font-style: italic } /* Comment.Hashbang */
.codehilite .cm { color: #408080; font-style: italic } /* Comment.Multiline */
.codehilite .cp { color: #BC7A00 } /* Comment.Preproc */
.codehilite .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */
.codehilite .c1 { color: #408080; font-style: italic } /* Comment.Single */
.codehilite .cs { color: #408080; font-style: italic } /* Comment.Special */
.codehilite .gd { color: #A00000 } /* Generic.Deleted */
.codehilite .ge { font-style: italic } /* Generic.Emph */
.codehilite .gr { color: #FF0000 } /* Generic.Error */
.codehilite .gh { color: #000080; font-weight: bold } /* Generic.Heading */
.codehilite .gi { color: #00A000 } /* Generic.Inserted */
.codehilite .go { color: #888888 } /* Generic.Output */
.codehilite .gp { color: #000080; font-weight: bold } /* Generic.Prompt */
.codehilite .gs { font-weight: bold } /* Generic.Strong */
.codehilite .gu { color: #800080; font-weight: bold } /* Generic.Subheading */
.codehilite .gt { color: #0044DD } /* Generic.Traceback */
.codehilite .kc { color: #008000; font-weight: bold } /* Keyword.Constant */
.codehilite .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */
.codehilite .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */
.codehilite .kp { color: #008000 } /* Keyword.Pseudo */
.codehilite .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */
.codehilite .kt { color: #B00040 } /* Keyword.Type */
.codehilite .m { color: #666666 } /* Literal.Number */
.codehilite .s { color: #BA2121 } /* Literal.String */
.codehilite .na { color: #7D9029 } /* Name.Attribute */
.codehilite .nb { color: #008000 } /* Name.Builtin */
.codehilite .nc { color: #0000FF; font-weight: bold } /* Name.Class */
.codehilite .no { color: #880000 } /* Name.Constant */
.codehilite .nd { color: #AA22FF } /* Name.Decorator */
.codehilite .ni { color: #999999; font-weight: bold } /* Name.Entity */
.codehilite .ne { color: #D2413A; font-weight: bold } /* Name.Exception */
.codehilite .nf { color: #0000FF } /* Name.Function */
.codehilite .nl { color: #A0A000 } /* Name.Label */
.codehilite .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */
.codehilite .nt { color: #008000; font-weight: bold } /* Name.Tag */
.codehilite .nv { color: #19177C } /* Name.Variable */
.codehilite .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */
.codehilite .w { color: #bbbbbb } /* Text.Whitespace */
.codehilite .mb { color: #666666 } /* Literal.Number.Bin */
.codehilite .mf { color: #666666 } /* Literal.Number.Float */
.codehilite .mh { color: #666666 } /* Literal.Number.Hex */
.codehilite .mi { color: #666666 } /* Literal.Number.Integer */
.codehilite .mo { color: #666666 } /* Literal.Number.Oct */
.codehilite .sb { color: #BA2121 } /* Literal.String.Backtick */
.codehilite .sc { color: #BA2121 } /* Literal.String.Char */
.codehilite .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */
.codehilite .s2 { color: #BA2121 } /* Literal.String.Double */
.codehilite .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */
.codehilite .sh { color: #BA2121 } /* Literal.String.Heredoc */
.codehilite .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */
.codehilite .sx { color: #008000 } /* Literal.String.Other */
.codehilite .sr { color: #BB6688 } /* Literal.String.Regex */
.codehilite .s1 { color: #BA2121 } /* Literal.String.Single */
.codehilite .ss { color: #19177C } /* Literal.String.Symbol */
.codehilite .bp { color: #008000 } /* Name.Builtin.Pseudo */
.codehilite .vc { color: #19177C } /* Name.Variable.Class */
.codehilite .vg { color: #19177C } /* Name.Variable.Global */
.codehilite .vi { color: #19177C } /* Name.Variable.Instance */
.codehilite .il { color: #666666 } /* Literal.Number.Integer.Long */
  </style>

  <style type="text/css">
  
/* ==========================================================================
   EXAMPLE Media Queries for Responsive Design.
   These examples override the primary ('mobile first') styles.
   Modify as content requires.
   ========================================================================== */

@media only screen and (min-width: 35em) {
    /* Style adjustments for viewports that meet the condition */
}

@media print,
       (-o-min-device-pixel-ratio: 5/4),
       (-webkit-min-device-pixel-ratio: 1.25),
       (min-resolution: 120dpi) {
    /* Style adjustments for high resolution devices */
}

/* ==========================================================================
   Print styles.
   Inlined to avoid required HTTP connection: h5bp.com/r
   ========================================================================== */

@media print {
    * {
        background: transparent !important;
        color: #000 !important; /* Black prints faster: h5bp.com/s */
        box-shadow: none !important;
        text-shadow: none !important;
    }

    a,
    a:visited {
        text-decoration: underline;
    }

    a[href]:after {
        content: " (" attr(href) ")";
    }

    abbr[title]:after {
        content: " (" attr(title) ")";
    }

    /*
     * Don't show links for images, or javascript/internal links
     */

    .ir a:after,
    a[href^="javascript:"]:after,
    a[href^="#"]:after {
        content: "";
    }

    pre,
    blockquote {
        border: 1px solid #999;
        page-break-inside: avoid;
    }

    thead {
        display: table-header-group; /* h5bp.com/t */
    }

    tr,
    img {
        page-break-inside: avoid;
    }

    img {
        max-width: 100% !important;
    }

    @page {
        margin: 0.5cm;
    }

    p,
    h2,
    h3 {
        orphans: 3;
        widows: 3;
    }

    h2,
    h3 {
        page-break-after: avoid;
    }
}

  </style>

  <script type="text/javascript">
  function toggle(id, $link) {
    $node = document.getElementById(id);
    if (!$node)
    return;
    if (!$node.style.display || $node.style.display == 'none') {
    $node.style.display = 'block';
    $link.innerHTML = 'Hide source &nequiv;';
    } else {
    $node.style.display = 'none';
    $link.innerHTML = 'Show source &equiv;';
    }
  }
  </script>
</head>
<body>
<a href="#" id="top">Top</a>

<div id="container">
    
  
  <div id="sidebar">
    <h1>Index</h1>
    <ul id="index">


    <li class="set"><h3><a href="#header-classes">Classes</a></h3>
      <ul>
        <li class="mono">
        <span class="class_name"><a href="#tensorbuilder.core.ApplicativeBase">ApplicativeBase</a></span>
        
          
  <ul>
    <li class="mono"><a href="#tensorbuilder.core.ApplicativeBase.__init__">__init__</a></li>
    <li class="mono"><a href="#tensorbuilder.core.ApplicativeBase.Builder">Builder</a></li>
    <li class="mono"><a href="#tensorbuilder.core.ApplicativeBase.compile">compile</a></li>
    <li class="mono"><a href="#tensorbuilder.core.ApplicativeBase.compose">compose</a></li>
    <li class="mono"><a href="#tensorbuilder.core.ApplicativeBase.copy">copy</a></li>
    <li class="mono"><a href="#tensorbuilder.core.ApplicativeBase.pipe">pipe</a></li>
    <li class="mono"><a href="#tensorbuilder.core.ApplicativeBase.register_method">register_method</a></li>
    <li class="mono"><a href="#tensorbuilder.core.ApplicativeBase.register_tensor_method">register_tensor_method</a></li>
  </ul>

        </li>
        <li class="mono">
        <span class="class_name"><a href="#tensorbuilder.core.BuilderBase">BuilderBase</a></span>
        
          
  <ul>
    <li class="mono"><a href="#tensorbuilder.core.BuilderBase.__init__">__init__</a></li>
    <li class="mono"><a href="#tensorbuilder.core.BuilderBase.BuilderTree">BuilderTree</a></li>
    <li class="mono"><a href="#tensorbuilder.core.BuilderBase.branch">branch</a></li>
    <li class="mono"><a href="#tensorbuilder.core.BuilderBase.copy">copy</a></li>
    <li class="mono"><a href="#tensorbuilder.core.BuilderBase.map">map</a></li>
    <li class="mono"><a href="#tensorbuilder.core.BuilderBase.register_map_method">register_map_method</a></li>
    <li class="mono"><a href="#tensorbuilder.core.BuilderBase.register_method">register_method</a></li>
    <li class="mono"><a href="#tensorbuilder.core.BuilderBase.store_on">store_on</a></li>
    <li class="mono"><a href="#tensorbuilder.core.BuilderBase.tensor">tensor</a></li>
    <li class="mono"><a href="#tensorbuilder.core.BuilderBase.then">then</a></li>
    <li class="mono"><a href="#tensorbuilder.core.BuilderBase.then_with">then_with</a></li>
  </ul>

        </li>
        <li class="mono">
        <span class="class_name"><a href="#tensorbuilder.core.BuilderTreeBase">BuilderTreeBase</a></span>
        
          
  <ul>
    <li class="mono"><a href="#tensorbuilder.core.BuilderTreeBase.__init__">__init__</a></li>
    <li class="mono"><a href="#tensorbuilder.core.BuilderTreeBase.Builder">Builder</a></li>
    <li class="mono"><a href="#tensorbuilder.core.BuilderTreeBase.builders">builders</a></li>
    <li class="mono"><a href="#tensorbuilder.core.BuilderTreeBase.copy">copy</a></li>
    <li class="mono"><a href="#tensorbuilder.core.BuilderTreeBase.extract">extract</a></li>
    <li class="mono"><a href="#tensorbuilder.core.BuilderTreeBase.map_each">map_each</a></li>
    <li class="mono"><a href="#tensorbuilder.core.BuilderTreeBase.reduce">reduce</a></li>
    <li class="mono"><a href="#tensorbuilder.core.BuilderTreeBase.register_method">register_method</a></li>
    <li class="mono"><a href="#tensorbuilder.core.BuilderTreeBase.register_reduce_method">register_reduce_method</a></li>
    <li class="mono"><a href="#tensorbuilder.core.BuilderTreeBase.tensors">tensors</a></li>
  </ul>

        </li>
      </ul>
    </li>

    <li class="set"><h3><a href="#header-submodules">Sub-modules</a></h3>
      <ul>
        <li class="mono"><a href="concrete_classes.m.html">tensorbuilder.core.concrete_classes</a></li>
        <li class="mono"><a href="utils.m.html">tensorbuilder.core.utils</a></li>
      </ul>
    </li>
    </ul>
  </div>

    <article id="content">
      
  

  


  <header id="section-intro">
  <h1 class="title"><span class="name">tensorbuilder.core</span> module</h1>
  
  
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-tensorbuilder.core', this);">Show source &equiv;</a></p>
  <div id="source-tensorbuilder.core" class="source">
    <div class="codehilite"><pre><span></span><span class="kn">from</span> <span class="nn">builders</span> <span class="kn">import</span> <span class="n">BuilderBase</span><span class="p">,</span> <span class="n">BuilderTreeBase</span>
<span class="kn">from</span> <span class="nn">applicative</span> <span class="kn">import</span> <span class="n">ApplicativeBase</span>
<span class="kn">import</span> <span class="nn">utils</span>
<span class="kn">import</span> <span class="nn">concrete_classes</span>

<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;BuilderBase&quot;</span><span class="p">,</span> <span class="s2">&quot;BuilderTreeBase&quot;</span><span class="p">,</span> <span class="s2">&quot;ApplicativeBase&quot;</span><span class="p">,</span> <span class="s2">&quot;utils&quot;</span><span class="p">,</span> <span class="s2">&quot;concrete_classes&quot;</span><span class="p">]</span>
</pre></div>

  </div>

  </header>

  <section id="section-items">


    <h2 class="section-title" id="header-classes">Classes</h2>
      
      <div class="item">
      <p id="tensorbuilder.core.ApplicativeBase" class="name">class <span class="ident">ApplicativeBase</span></p>
      
  
    <div class="desc"><p>An <a href="http://learnyouahaskell.com/functors-applicative-functors-and-monoids">Applicative</a> is an object who wraps around a function and posses the means to apply it.</p>
<p>The <code>Applicative</code> class contains a inner field <code>f</code> that must be a function, internal methods rely on this fact to give you the nice syntax of the DSL. The <code>Applicative</code> class is also a function, meaning it implements the <code>__call__</code> method, which very simply delegates the computation to the function it contains.</p>
<blockquote>
<p><strong>Note:</strong> The <code>tb</code> object with is contains the whole TensorBuilder API is an Applicative itself, it contians the identity function.</p>
</blockquote>
<p><strong>DSL</strong></p>
<p>Check out the description of the DSL <a href="https://cgarciae.gitbooks.io/tensorbuilder/content/dsl/">here</a>.</p>
<p><strong>Properties</strong></p>
<p>Many methods registered/patched by TensorBuilder onto <code>Applicative</code> actually use <code>tensorbuilder.core.applicative.Applicative.compose</code> internally, therefore, an expression of the DSL like this</p>
<div class="codehilite"><pre><span></span>(tb.softmax(),
tb.dropout(keep_prob),
tb.relu_layer(10)) # Notice the begging and ending &#39;()&#39; tuple parenthesis
</pre></div>


<p>is equivalent to this</p>
<div class="codehilite"><pre><span></span>tb.softmax()
.dropout(keep_prob),
.relu_layer(10)
</pre></div></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-tensorbuilder.core.ApplicativeBase', this);">Show source &equiv;</a></p>
  <div id="source-tensorbuilder.core.ApplicativeBase" class="source">
    <div class="codehilite"><pre><span></span><span class="k">class</span> <span class="nc">ApplicativeBase</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    An [Applicative](http://learnyouahaskell.com/functors-applicative-functors-and-monoids) is an object who wraps around a function and posses the means to apply it.</span>

<span class="sd">    The `Applicative` class contains a inner field `f` that must be a function, internal methods rely on this fact to give you the nice syntax of the DSL. The `Applicative` class is also a function, meaning it implements the `__call__` method, which very simply delegates the computation to the function it contains.</span>

<span class="sd">    &gt; **Note:** The `tb` object with is contains the whole TensorBuilder API is an Applicative itself, it contians the identity function.</span>

<span class="sd">    **DSL**</span>

<span class="sd">    Check out the description of the DSL [here](https://cgarciae.gitbooks.io/tensorbuilder/content/dsl/).</span>

<span class="sd">    **Properties**</span>

<span class="sd">    Many methods registered/patched by TensorBuilder onto `Applicative` actually use `tensorbuilder.core.applicative.Applicative.compose` internally, therefore, an expression of the DSL like this</span>

<span class="sd">        (tb.softmax(),</span>
<span class="sd">        tb.dropout(keep_prob),</span>
<span class="sd">        tb.relu_layer(10)) # Notice the begging and ending &#39;()&#39; tuple parenthesis</span>

<span class="sd">    is equivalent to this</span>

<span class="sd">        tb.softmax()</span>
<span class="sd">        .dropout(keep_prob),</span>
<span class="sd">        .relu_layer(10)</span>


<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__metaclass__</span> <span class="o">=</span> <span class="n">ABCMeta</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">f</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ApplicativeBase</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">f</span> <span class="o">=</span> <span class="n">f</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        A function of type `a -&gt; b`.</span>
<span class="sd">        &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">Builder</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensor</span><span class="p">):</span>
        <span class="k">pass</span>


    <span class="k">def</span> <span class="nf">_unit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">f</span><span class="p">):</span>
        <span class="s2">&quot;Monadic unit, also known as `return`&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">__class__</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">copy</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Returns a compy of the applicative&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_unit</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">f</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">f</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">compose</span><span class="p">(</span><span class="n">app</span><span class="p">,</span> <span class="n">g</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Takes in a function `g` and composes it with `tensorbuilder.core.Applicative.f` as `g o f`. All \*args and \*\* are forwarded to g. This is an essential method since most registered methods use this.</span>

<span class="sd">        **Arguments**</span>

<span class="sd">        * `g`: A function</span>
<span class="sd">        * All \*args and \*\* are forwarded to `g`</span>

<span class="sd">        **Return**</span>

<span class="sd">        Applicative</span>

<span class="sd">        **Examples**</span>

<span class="sd">            import tensorflow as tf</span>
<span class="sd">            from tensorbuilder import tb</span>


<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">app</span><span class="o">.</span><span class="n">_unit</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">g</span><span class="p">(</span><span class="n">app</span><span class="o">.</span><span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">pipe</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">builder</span><span class="p">,</span> <span class="o">*</span><span class="n">ast</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        `pipe` takes in a `builder` of type `Builder`, `BuilderTree` or `Tensor` preferably and an object `ast` which must be part of the domain of the DSL, and compiles `ast` to a function of type `Builder -&gt; Builder` and applies it to the input `builder`. All \*args after `builder` are taken as a tuple, therefore, it makes it easier to define an initial tuple `()` element to define a sequential operation.</span>

<span class="sd">        **Arguments**</span>

<span class="sd">        * `builder`: a `Builder`, `BuilderTree` or `Tensor` preferably.</span>
<span class="sd">        * `*ast`: a sequence of elements of the DSL.</span>

<span class="sd">        **Return**</span>

<span class="sd">        An object with the result of the computation, probable types: `Tensor | Builder | BuilderTree | list(Tensor) |  `</span>

<span class="sd">        **Examples**</span>

<span class="sd">            import tensorflow as tf</span>
<span class="sd">            from tensorbuilder import tb</span>

<span class="sd">            x = placeholder(tf.float32, shape=[None, 10])</span>

<span class="sd">            h = tb.pipe(</span>
<span class="sd">                x,</span>
<span class="sd">                [</span>
<span class="sd">                    { tf.device(&quot;/gpu:0&quot;):</span>
<span class="sd">                        tb.relu_layer(20)</span>
<span class="sd">                    }</span>
<span class="sd">                ,</span>
<span class="sd">                    { tf.device(&quot;/gpu:1&quot;):</span>
<span class="sd">                        tb.sigmoid_layer(20)</span>
<span class="sd">                    }</span>
<span class="sd">                ,</span>
<span class="sd">                    { tf.device(&quot;/cpu:0&quot;):</span>
<span class="sd">                        tb.tanh_layer(20)</span>
<span class="sd">                    }</span>
<span class="sd">                ],</span>
<span class="sd">                tb.relu_layer(10)</span>
<span class="sd">                .tensor()</span>
<span class="sd">            )</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">f</span> <span class="o">=</span> <span class="n">_compile</span><span class="p">(</span><span class="n">ast</span><span class="p">)</span>

        <span class="c1">#if the input is a Tensor, create a Builder</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">builder</span><span class="p">)</span> <span class="ow">is</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span> <span class="ow">or</span> <span class="nb">type</span><span class="p">(</span><span class="n">builder</span><span class="p">)</span> <span class="ow">is</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">:</span>
            <span class="n">builder</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Builder</span><span class="p">(</span><span class="n">builder</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">f</span><span class="p">(</span><span class="n">builder</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">compile</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">ast</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        `compile` an object `ast` which must be part of the domain of the DSL and returns function. It applies the rules of the DSL to create an actual Python function that does what you intend. Normally you will just use pipe, which not only compiles the DSL it actually performs the computation to a given Tensor/Builder, however, it you are building and API this might be useful since you can create a function from an AST which can itself be used as an element of another AST since final elements of the DSL are functions.</span>

<span class="sd">        **Arguments**</span>

<span class="sd">        * `*ast`: a sequence of elements of the DSL.</span>

<span class="sd">        **Return**</span>

<span class="sd">        A function</span>

<span class="sd">        **Examples**</span>

<span class="sd">            import tensorflow as tf</span>
<span class="sd">            from tensorbuilder import tb</span>

<span class="sd">            x = placeholder(tf.float32, shape=[None, 10])</span>

<span class="sd">            f = tb.compile(</span>
<span class="sd">                tb.build, #accept a Tensor as a parameter and create a builder so you can use the rest of the methods</span>
<span class="sd">                [</span>
<span class="sd">                    { tf.device(&quot;/gpu:0&quot;):</span>
<span class="sd">                        tb.relu_layer(20)</span>
<span class="sd">                    }</span>
<span class="sd">                ,</span>
<span class="sd">                    { tf.device(&quot;/gpu:1&quot;):</span>
<span class="sd">                        tb.sigmoid_layer(20)</span>
<span class="sd">                    }</span>
<span class="sd">                ,</span>
<span class="sd">                    { tf.device(&quot;/cpu:0&quot;):</span>
<span class="sd">                        tb.tanh_layer(20)</span>
<span class="sd">                    }</span>
<span class="sd">                ],</span>
<span class="sd">                tb.relu_layer(10)</span>
<span class="sd">                .tensor()</span>
<span class="sd">            )</span>

<span class="sd">            h = f(x)</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">_compile</span><span class="p">(</span><span class="n">ast</span><span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">register_method</span><span class="p">(</span><span class="n">cls</span><span class="p">,</span> <span class="n">fn</span><span class="p">,</span> <span class="n">library_path</span><span class="p">,</span> <span class="n">alias</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">doc</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This method enables you to register any function `fn` that takes an Applicative as its first argument as a method of the Builder class.</span>

<span class="sd">        **Arguments**</span>

<span class="sd">        * `fn`: a function that atleast takes an Applicative as its first argument.</span>
<span class="sd">        * `library_path`: the route of the librar from which this function was taken, used for documentation purposes.</span>
<span class="sd">        * `alias`: allows you to specify the name of the method, it will take the name of the function if its `None`.</span>
<span class="sd">        * `doc`: the documentation for the method, if `None` a predefied documentation will be generated based on the documentation of `fn`.</span>

<span class="sd">        **Return**</span>

<span class="sd">        `None`</span>

<span class="sd">        **Examples**</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">fn_signature</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">get_method_sig</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span>
     	<span class="n">fn_docs</span> <span class="o">=</span> <span class="n">inspect</span><span class="o">.</span><span class="n">getdoc</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span>
        <span class="n">original_name</span> <span class="o">=</span> <span class="n">fn</span><span class="o">.</span><span class="n">__name__</span>
        <span class="n">name</span> <span class="o">=</span> <span class="n">alias</span> <span class="k">if</span> <span class="n">alias</span> <span class="k">else</span> <span class="n">original_name</span>

        <span class="n">fn</span><span class="o">.</span><span class="n">__name__</span> <span class="o">=</span> <span class="n">name</span>
        <span class="n">fn</span><span class="o">.</span><span class="n">__doc__</span> <span class="o">=</span> <span class="n">doc</span> <span class="k">if</span> <span class="n">doc</span> <span class="k">else</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">        THIS METHOD IS AUTOMATICALLY GENERATED</span>

<span class="s2">        This method accepts the same arguments as `{3}.{0}`</span>

<span class="s2">        ** Documentation from `{3}.{0}`**</span>

<span class="s2">            def {1}</span>
<span class="s2">        &quot;&quot;&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">fn_signature</span><span class="p">,</span> <span class="n">fn</span><span class="o">.</span><span class="n">__doc__</span><span class="p">,</span> <span class="n">library_path</span><span class="p">)</span>


        <span class="nb">setattr</span><span class="p">(</span><span class="n">cls</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">fn</span><span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">register_tensor_method</span><span class="p">(</span><span class="n">cls</span><span class="p">,</span> <span class="n">fn</span><span class="p">,</span> <span class="n">library_path</span><span class="p">,</span> <span class="n">alias</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">doc</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This method enables you to register any function `fn` that takes an tensor as its first argument as a method of the Builder and Applicative class.</span>

<span class="sd">        **Arguments**</span>

<span class="sd">        * `fn`: a function that atleast takes an Tensor as its first argument.</span>
<span class="sd">        * `library_path`: the route of the librar from which this function was taken, used for documentation purposes.</span>
<span class="sd">        * `alias`: allows you to specify the name of the method, it will take the name of the function if its `None`.</span>
<span class="sd">        * `doc`: the documentation for the method, if `None` a predefied documentation will be generated based on the documentation of `fn`.</span>

<span class="sd">        **Return**</span>

<span class="sd">        `None`</span>

<span class="sd">        **Examples**</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">original_name</span> <span class="o">=</span> <span class="n">fn</span><span class="o">.</span><span class="n">__name__</span>
        <span class="n">name</span> <span class="o">=</span> <span class="n">alias</span> <span class="k">if</span> <span class="n">alias</span> <span class="k">else</span> <span class="n">original_name</span>
        <span class="n">method</span> <span class="o">=</span> <span class="n">get_app_method</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>

        <span class="n">cls</span><span class="o">.</span><span class="n">register_method</span><span class="p">(</span><span class="n">method</span><span class="p">,</span> <span class="n">library_path</span><span class="p">,</span> <span class="n">alias</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">doc</span><span class="o">=</span><span class="n">doc</span><span class="p">)</span>
</pre></div>

  </div>
</div>


      <div class="class">
          <h3>Ancestors (in MRO)</h3>
          <ul class="class_list">
          <li><a href="#tensorbuilder.core.ApplicativeBase">ApplicativeBase</a></li>
          <li>__builtin__.object</li>
          </ul>
          <h3>Instance variables</h3>
            <div class="item">
            <p id="tensorbuilder.core.ApplicativeBase.f" class="name">var <span class="ident">f</span></p>
            

            
  
    <div class="desc"><p>A function of type <code>a -&gt; b</code>.</p></div>
  <div class="source_cont">
</div>

            </div>
          <h3>Methods</h3>
            
  <div class="item">
    <div class="name def" id="tensorbuilder.core.ApplicativeBase.__init__">
    <p>def <span class="ident">__init__</span>(</p><p>self, f)</p>
    </div>
    

    
  
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-tensorbuilder.core.ApplicativeBase.__init__', this);">Show source &equiv;</a></p>
  <div id="source-tensorbuilder.core.ApplicativeBase.__init__" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">f</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">ApplicativeBase</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">f</span> <span class="o">=</span> <span class="n">f</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A function of type `a -&gt; b`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="tensorbuilder.core.ApplicativeBase.Builder">
    <p>def <span class="ident">Builder</span>(</p><p>self, tensor)</p>
    </div>
    

    
  
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-tensorbuilder.core.ApplicativeBase.Builder', this);">Show source &equiv;</a></p>
  <div id="source-tensorbuilder.core.ApplicativeBase.Builder" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">Builder</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensor</span><span class="p">):</span>
    <span class="k">pass</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="tensorbuilder.core.ApplicativeBase.compile">
    <p>def <span class="ident">compile</span>(</p><p>self, *ast)</p>
    </div>
    

    
  
    <div class="desc"><p><code>compile</code> an object <code>ast</code> which must be part of the domain of the DSL and returns function. It applies the rules of the DSL to create an actual Python function that does what you intend. Normally you will just use pipe, which not only compiles the DSL it actually performs the computation to a given Tensor/Builder, however, it you are building and API this might be useful since you can create a function from an AST which can itself be used as an element of another AST since final elements of the DSL are functions.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>*ast</code>: a sequence of elements of the DSL.</li>
</ul>
<p><strong>Return</strong></p>
<p>A function</p>
<p><strong>Examples</strong></p>
<div class="codehilite"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>
<span class="kn">from</span> <span class="nn">tensorbuilder</span> <span class="kn">import</span> <span class="n">tb</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">10</span><span class="p">])</span>

<span class="n">f</span> <span class="o">=</span> <span class="n">tb</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span>
    <span class="n">tb</span><span class="o">.</span><span class="n">build</span><span class="p">,</span> <span class="c1">#accept a Tensor as a parameter and create a builder so you can use the rest of the methods</span>
    <span class="p">[</span>
        <span class="p">{</span> <span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;/gpu:0&quot;</span><span class="p">):</span>
            <span class="n">tb</span><span class="o">.</span><span class="n">relu_layer</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span>
        <span class="p">}</span>
    <span class="p">,</span>
        <span class="p">{</span> <span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;/gpu:1&quot;</span><span class="p">):</span>
            <span class="n">tb</span><span class="o">.</span><span class="n">sigmoid_layer</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span>
        <span class="p">}</span>
    <span class="p">,</span>
        <span class="p">{</span> <span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;/cpu:0&quot;</span><span class="p">):</span>
            <span class="n">tb</span><span class="o">.</span><span class="n">tanh_layer</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span>
        <span class="p">}</span>
    <span class="p">],</span>
    <span class="n">tb</span><span class="o">.</span><span class="n">relu_layer</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
    <span class="o">.</span><span class="n">tensor</span><span class="p">()</span>
<span class="p">)</span>

<span class="n">h</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-tensorbuilder.core.ApplicativeBase.compile', this);">Show source &equiv;</a></p>
  <div id="source-tensorbuilder.core.ApplicativeBase.compile" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">compile</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">ast</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    `compile` an object `ast` which must be part of the domain of the DSL and returns function. It applies the rules of the DSL to create an actual Python function that does what you intend. Normally you will just use pipe, which not only compiles the DSL it actually performs the computation to a given Tensor/Builder, however, it you are building and API this might be useful since you can create a function from an AST which can itself be used as an element of another AST since final elements of the DSL are functions.</span>
<span class="sd">    **Arguments**</span>
<span class="sd">    * `*ast`: a sequence of elements of the DSL.</span>
<span class="sd">    **Return**</span>
<span class="sd">    A function</span>
<span class="sd">    **Examples**</span>
<span class="sd">        import tensorflow as tf</span>
<span class="sd">        from tensorbuilder import tb</span>
<span class="sd">        x = placeholder(tf.float32, shape=[None, 10])</span>
<span class="sd">        f = tb.compile(</span>
<span class="sd">            tb.build, #accept a Tensor as a parameter and create a builder so you can use the rest of the methods</span>
<span class="sd">            [</span>
<span class="sd">                { tf.device(&quot;/gpu:0&quot;):</span>
<span class="sd">                    tb.relu_layer(20)</span>
<span class="sd">                }</span>
<span class="sd">            ,</span>
<span class="sd">                { tf.device(&quot;/gpu:1&quot;):</span>
<span class="sd">                    tb.sigmoid_layer(20)</span>
<span class="sd">                }</span>
<span class="sd">            ,</span>
<span class="sd">                { tf.device(&quot;/cpu:0&quot;):</span>
<span class="sd">                    tb.tanh_layer(20)</span>
<span class="sd">                }</span>
<span class="sd">            ],</span>
<span class="sd">            tb.relu_layer(10)</span>
<span class="sd">            .tensor()</span>
<span class="sd">        )</span>
<span class="sd">        h = f(x)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_compile</span><span class="p">(</span><span class="n">ast</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="tensorbuilder.core.ApplicativeBase.compose">
    <p>def <span class="ident">compose</span>(</p><p>app, g, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>Takes in a function <code>g</code> and composes it with <code>tensorbuilder.core.Applicative.f</code> as <code>g o f</code>. All *args and ** are forwarded to g. This is an essential method since most registered methods use this.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>g</code>: A function</li>
<li>All *args and ** are forwarded to <code>g</code></li>
</ul>
<p><strong>Return</strong></p>
<p>Applicative</p>
<p><strong>Examples</strong></p>
<div class="codehilite"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>
<span class="kn">from</span> <span class="nn">tensorbuilder</span> <span class="kn">import</span> <span class="n">tb</span>
</pre></div></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-tensorbuilder.core.ApplicativeBase.compose', this);">Show source &equiv;</a></p>
  <div id="source-tensorbuilder.core.ApplicativeBase.compose" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">compose</span><span class="p">(</span><span class="n">app</span><span class="p">,</span> <span class="n">g</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Takes in a function `g` and composes it with `tensorbuilder.core.Applicative.f` as `g o f`. All \*args and \*\* are forwarded to g. This is an essential method since most registered methods use this.</span>
<span class="sd">    **Arguments**</span>
<span class="sd">    * `g`: A function</span>
<span class="sd">    * All \*args and \*\* are forwarded to `g`</span>
<span class="sd">    **Return**</span>
<span class="sd">    Applicative</span>
<span class="sd">    **Examples**</span>
<span class="sd">        import tensorflow as tf</span>
<span class="sd">        from tensorbuilder import tb</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">app</span><span class="o">.</span><span class="n">_unit</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">g</span><span class="p">(</span><span class="n">app</span><span class="o">.</span><span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">))</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="tensorbuilder.core.ApplicativeBase.copy">
    <p>def <span class="ident">copy</span>(</p><p>self)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns a compy of the applicative</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-tensorbuilder.core.ApplicativeBase.copy', this);">Show source &equiv;</a></p>
  <div id="source-tensorbuilder.core.ApplicativeBase.copy" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">copy</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns a compy of the applicative&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_unit</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">f</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="tensorbuilder.core.ApplicativeBase.pipe">
    <p>def <span class="ident">pipe</span>(</p><p>self, builder, *ast)</p>
    </div>
    

    
  
    <div class="desc"><p><code>pipe</code> takes in a <code>builder</code> of type <code>Builder</code>, <code>BuilderTree</code> or <code>Tensor</code> preferably and an object <code>ast</code> which must be part of the domain of the DSL, and compiles <code>ast</code> to a function of type <code>Builder -&gt; Builder</code> and applies it to the input <code>builder</code>. All *args after <code>builder</code> are taken as a tuple, therefore, it makes it easier to define an initial tuple <code>()</code> element to define a sequential operation.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>builder</code>: a <code>Builder</code>, <code>BuilderTree</code> or <code>Tensor</code> preferably.</li>
<li><code>*ast</code>: a sequence of elements of the DSL.</li>
</ul>
<p><strong>Return</strong></p>
<p>An object with the result of the computation, probable types: <code>Tensor | Builder | BuilderTree | list(Tensor) |</code></p>
<p><strong>Examples</strong></p>
<div class="codehilite"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>
<span class="kn">from</span> <span class="nn">tensorbuilder</span> <span class="kn">import</span> <span class="n">tb</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">10</span><span class="p">])</span>

<span class="n">h</span> <span class="o">=</span> <span class="n">tb</span><span class="o">.</span><span class="n">pipe</span><span class="p">(</span>
    <span class="n">x</span><span class="p">,</span>
    <span class="p">[</span>
        <span class="p">{</span> <span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;/gpu:0&quot;</span><span class="p">):</span>
            <span class="n">tb</span><span class="o">.</span><span class="n">relu_layer</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span>
        <span class="p">}</span>
    <span class="p">,</span>
        <span class="p">{</span> <span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;/gpu:1&quot;</span><span class="p">):</span>
            <span class="n">tb</span><span class="o">.</span><span class="n">sigmoid_layer</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span>
        <span class="p">}</span>
    <span class="p">,</span>
        <span class="p">{</span> <span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;/cpu:0&quot;</span><span class="p">):</span>
            <span class="n">tb</span><span class="o">.</span><span class="n">tanh_layer</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span>
        <span class="p">}</span>
    <span class="p">],</span>
    <span class="n">tb</span><span class="o">.</span><span class="n">relu_layer</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
    <span class="o">.</span><span class="n">tensor</span><span class="p">()</span>
<span class="p">)</span>
</pre></div></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-tensorbuilder.core.ApplicativeBase.pipe', this);">Show source &equiv;</a></p>
  <div id="source-tensorbuilder.core.ApplicativeBase.pipe" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">pipe</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">builder</span><span class="p">,</span> <span class="o">*</span><span class="n">ast</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    `pipe` takes in a `builder` of type `Builder`, `BuilderTree` or `Tensor` preferably and an object `ast` which must be part of the domain of the DSL, and compiles `ast` to a function of type `Builder -&gt; Builder` and applies it to the input `builder`. All \*args after `builder` are taken as a tuple, therefore, it makes it easier to define an initial tuple `()` element to define a sequential operation.</span>
<span class="sd">    **Arguments**</span>
<span class="sd">    * `builder`: a `Builder`, `BuilderTree` or `Tensor` preferably.</span>
<span class="sd">    * `*ast`: a sequence of elements of the DSL.</span>
<span class="sd">    **Return**</span>
<span class="sd">    An object with the result of the computation, probable types: `Tensor | Builder | BuilderTree | list(Tensor) |  `</span>
<span class="sd">    **Examples**</span>
<span class="sd">        import tensorflow as tf</span>
<span class="sd">        from tensorbuilder import tb</span>
<span class="sd">        x = placeholder(tf.float32, shape=[None, 10])</span>
<span class="sd">        h = tb.pipe(</span>
<span class="sd">            x,</span>
<span class="sd">            [</span>
<span class="sd">                { tf.device(&quot;/gpu:0&quot;):</span>
<span class="sd">                    tb.relu_layer(20)</span>
<span class="sd">                }</span>
<span class="sd">            ,</span>
<span class="sd">                { tf.device(&quot;/gpu:1&quot;):</span>
<span class="sd">                    tb.sigmoid_layer(20)</span>
<span class="sd">                }</span>
<span class="sd">            ,</span>
<span class="sd">                { tf.device(&quot;/cpu:0&quot;):</span>
<span class="sd">                    tb.tanh_layer(20)</span>
<span class="sd">                }</span>
<span class="sd">            ],</span>
<span class="sd">            tb.relu_layer(10)</span>
<span class="sd">            .tensor()</span>
<span class="sd">        )</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">f</span> <span class="o">=</span> <span class="n">_compile</span><span class="p">(</span><span class="n">ast</span><span class="p">)</span>
    <span class="c1">#if the input is a Tensor, create a Builder</span>
    <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">builder</span><span class="p">)</span> <span class="ow">is</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span> <span class="ow">or</span> <span class="nb">type</span><span class="p">(</span><span class="n">builder</span><span class="p">)</span> <span class="ow">is</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">:</span>
        <span class="n">builder</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Builder</span><span class="p">(</span><span class="n">builder</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">f</span><span class="p">(</span><span class="n">builder</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="tensorbuilder.core.ApplicativeBase.register_method">
    <p>def <span class="ident">register_method</span>(</p><p>cls, fn, library_path, alias=None, doc=None)</p>
    </div>
    

    
  
    <div class="desc"><p>This method enables you to register any function <code>fn</code> that takes an Applicative as its first argument as a method of the Builder class.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>fn</code>: a function that atleast takes an Applicative as its first argument.</li>
<li><code>library_path</code>: the route of the librar from which this function was taken, used for documentation purposes.</li>
<li><code>alias</code>: allows you to specify the name of the method, it will take the name of the function if its <code>None</code>.</li>
<li><code>doc</code>: the documentation for the method, if <code>None</code> a predefied documentation will be generated based on the documentation of <code>fn</code>.</li>
</ul>
<p><strong>Return</strong></p>
<p><code>None</code></p>
<p><strong>Examples</strong></p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-tensorbuilder.core.ApplicativeBase.register_method', this);">Show source &equiv;</a></p>
  <div id="source-tensorbuilder.core.ApplicativeBase.register_method" class="source">
    <div class="codehilite"><pre><span></span><span class="nd">@classmethod</span>
<span class="k">def</span> <span class="nf">register_method</span><span class="p">(</span><span class="n">cls</span><span class="p">,</span> <span class="n">fn</span><span class="p">,</span> <span class="n">library_path</span><span class="p">,</span> <span class="n">alias</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">doc</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This method enables you to register any function `fn` that takes an Applicative as its first argument as a method of the Builder class.</span>
<span class="sd">    **Arguments**</span>
<span class="sd">    * `fn`: a function that atleast takes an Applicative as its first argument.</span>
<span class="sd">    * `library_path`: the route of the librar from which this function was taken, used for documentation purposes.</span>
<span class="sd">    * `alias`: allows you to specify the name of the method, it will take the name of the function if its `None`.</span>
<span class="sd">    * `doc`: the documentation for the method, if `None` a predefied documentation will be generated based on the documentation of `fn`.</span>
<span class="sd">    **Return**</span>
<span class="sd">    `None`</span>
<span class="sd">    **Examples**</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">fn_signature</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">get_method_sig</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span>
 	<span class="n">fn_docs</span> <span class="o">=</span> <span class="n">inspect</span><span class="o">.</span><span class="n">getdoc</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span>
    <span class="n">original_name</span> <span class="o">=</span> <span class="n">fn</span><span class="o">.</span><span class="n">__name__</span>
    <span class="n">name</span> <span class="o">=</span> <span class="n">alias</span> <span class="k">if</span> <span class="n">alias</span> <span class="k">else</span> <span class="n">original_name</span>
    <span class="n">fn</span><span class="o">.</span><span class="n">__name__</span> <span class="o">=</span> <span class="n">name</span>
    <span class="n">fn</span><span class="o">.</span><span class="n">__doc__</span> <span class="o">=</span> <span class="n">doc</span> <span class="k">if</span> <span class="n">doc</span> <span class="k">else</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">    THIS METHOD IS AUTOMATICALLY GENERATED</span>
<span class="s2">    This method accepts the same arguments as `{3}.{0}`</span>
<span class="s2">    ** Documentation from `{3}.{0}`**</span>
<span class="s2">        def {1}</span>
<span class="s2">    &quot;&quot;&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">fn_signature</span><span class="p">,</span> <span class="n">fn</span><span class="o">.</span><span class="n">__doc__</span><span class="p">,</span> <span class="n">library_path</span><span class="p">)</span>
    <span class="nb">setattr</span><span class="p">(</span><span class="n">cls</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">fn</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="tensorbuilder.core.ApplicativeBase.register_tensor_method">
    <p>def <span class="ident">register_tensor_method</span>(</p><p>cls, fn, library_path, alias=None, doc=None)</p>
    </div>
    

    
  
    <div class="desc"><p>This method enables you to register any function <code>fn</code> that takes an tensor as its first argument as a method of the Builder and Applicative class.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>fn</code>: a function that atleast takes an Tensor as its first argument.</li>
<li><code>library_path</code>: the route of the librar from which this function was taken, used for documentation purposes.</li>
<li><code>alias</code>: allows you to specify the name of the method, it will take the name of the function if its <code>None</code>.</li>
<li><code>doc</code>: the documentation for the method, if <code>None</code> a predefied documentation will be generated based on the documentation of <code>fn</code>.</li>
</ul>
<p><strong>Return</strong></p>
<p><code>None</code></p>
<p><strong>Examples</strong></p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-tensorbuilder.core.ApplicativeBase.register_tensor_method', this);">Show source &equiv;</a></p>
  <div id="source-tensorbuilder.core.ApplicativeBase.register_tensor_method" class="source">
    <div class="codehilite"><pre><span></span><span class="nd">@classmethod</span>
<span class="k">def</span> <span class="nf">register_tensor_method</span><span class="p">(</span><span class="n">cls</span><span class="p">,</span> <span class="n">fn</span><span class="p">,</span> <span class="n">library_path</span><span class="p">,</span> <span class="n">alias</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">doc</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This method enables you to register any function `fn` that takes an tensor as its first argument as a method of the Builder and Applicative class.</span>
<span class="sd">    **Arguments**</span>
<span class="sd">    * `fn`: a function that atleast takes an Tensor as its first argument.</span>
<span class="sd">    * `library_path`: the route of the librar from which this function was taken, used for documentation purposes.</span>
<span class="sd">    * `alias`: allows you to specify the name of the method, it will take the name of the function if its `None`.</span>
<span class="sd">    * `doc`: the documentation for the method, if `None` a predefied documentation will be generated based on the documentation of `fn`.</span>
<span class="sd">    **Return**</span>
<span class="sd">    `None`</span>
<span class="sd">    **Examples**</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">original_name</span> <span class="o">=</span> <span class="n">fn</span><span class="o">.</span><span class="n">__name__</span>
    <span class="n">name</span> <span class="o">=</span> <span class="n">alias</span> <span class="k">if</span> <span class="n">alias</span> <span class="k">else</span> <span class="n">original_name</span>
    <span class="n">method</span> <span class="o">=</span> <span class="n">get_app_method</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
    <span class="n">cls</span><span class="o">.</span><span class="n">register_method</span><span class="p">(</span><span class="n">method</span><span class="p">,</span> <span class="n">library_path</span><span class="p">,</span> <span class="n">alias</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">doc</span><span class="o">=</span><span class="n">doc</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
      </div>
      </div>
      
      <div class="item">
      <p id="tensorbuilder.core.BuilderBase" class="name">class <span class="ident">BuilderBase</span></p>
      
  
    <div class="desc"><p>The Builder class is a wrapper around a Tensor. Most of its method are immutable, that is, they don't modify the caller object but always return a new builder.</p>
<p>This class is a Functor because it has the <code>map</code> method, to be a Monad is only missing the <code>bind</code> method which is trivial to implement. This means that even if its expected that the inner element contained within a Builder is a Tensor, it can actually contian anything and you may use this to your advantage.</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-tensorbuilder.core.BuilderBase', this);">Show source &equiv;</a></p>
  <div id="source-tensorbuilder.core.BuilderBase" class="source">
    <div class="codehilite"><pre><span></span><span class="k">class</span> <span class="nc">BuilderBase</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The Builder class is a wrapper around a Tensor. Most of its method are immutable, that is, they don&#39;t modify the caller object but always return a new builder.</span>

<span class="sd">    This class is a Functor because it has the `map` method, to be a Monad is only missing the `bind` method which is trivial to implement. This means that even if its expected that the inner element contained within a Builder is a Tensor, it can actually contian anything and you may use this to your advantage.</span>


<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__metaclass__</span> <span class="o">=</span> <span class="n">ABCMeta</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensor</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">BuilderBase</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_tensor</span> <span class="o">=</span> <span class="n">tensor</span>
        <span class="sd">&quot;&quot;&quot;A `tensorflow` Tensor.&quot;&quot;&quot;</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">BuilderTree</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">builder_iterable</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">tensor</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="s2">&quot;Returns the Tensor contianed by the Builder&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tensor</span>

    <span class="k">def</span> <span class="nf">copy</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Returns a copy of this Builder&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_unit</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tensor</span><span class="p">())</span>

    <span class="k">def</span> <span class="nf">_unit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensor</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">__class__</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">store_on</span><span class="p">(</span><span class="n">builder</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="n">other</span><span class="o">.</span><span class="n">_tensor</span> <span class="o">=</span> <span class="n">builder</span><span class="o">.</span><span class="n">tensor</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">builder</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">register_method</span><span class="p">(</span><span class="n">cls</span><span class="p">,</span> <span class="n">fn</span><span class="p">,</span> <span class="n">library_path</span><span class="p">,</span> <span class="n">alias</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">doc</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This method enables you to register any function `fn` that takes a Builder as its first argument as a method of the Builder class.</span>

<span class="sd">        **Arguments**</span>

<span class="sd">        * `fn`: a function that atleast takes a Builder as its first argument.</span>
<span class="sd">        * `library_path`: the route of the librar from which this function was taken, used for documentation purposes.</span>
<span class="sd">        * `alias`: allows you to specify the name of the method, it will take the name of the function if its `None`.</span>
<span class="sd">        * `doc`: the documentation for the method, if `None` a predefied documentation will be generated based on the documentation of `fn`.</span>

<span class="sd">        **Return**</span>

<span class="sd">        `None`</span>

<span class="sd">        **Examples**</span>

<span class="sd">        In this example we will create a funtion and register it as a method called `relu_dropout_layer`</span>

<span class="sd">            import tensorflow as tf</span>
<span class="sd">            from tensorbuilder import tb</span>


<span class="sd">            def relu_dropout(builder, size, keep_prob):</span>
<span class="sd">                \&quot;\&quot;\&quot;Fully connect to a relu layer of size `size` and apply dropout with `keep_prob`\&quot;\&quot;\&quot;</span>
<span class="sd">                return (</span>
<span class="sd">                    builder.map(tf.contrib.layers.fully_connected, size)</span>
<span class="sd">                    .map(tf.nn.relu)</span>
<span class="sd">                    .map(tf.nn.dropout, keep_prob)</span>
<span class="sd">                )</span>

<span class="sd">            tb.Builder.register_method(relu_dropout_layer, &quot;my.lib&quot;, alias=&quot;relu_dropout_layer&quot;)</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">fn_signature</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">get_method_sig</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span>
     	<span class="n">fn_docs</span> <span class="o">=</span> <span class="n">inspect</span><span class="o">.</span><span class="n">getdoc</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span>
        <span class="n">original_name</span> <span class="o">=</span> <span class="n">fn</span><span class="o">.</span><span class="n">__name__</span>
        <span class="n">name</span> <span class="o">=</span> <span class="n">alias</span> <span class="k">if</span> <span class="n">alias</span> <span class="k">else</span> <span class="n">original_name</span>

        <span class="n">fn</span><span class="o">.</span><span class="n">__name__</span> <span class="o">=</span> <span class="n">name</span>
        <span class="n">fn</span><span class="o">.</span><span class="n">__doc__</span> <span class="o">=</span> <span class="n">doc</span> <span class="k">if</span> <span class="n">doc</span> <span class="k">else</span> <span class="n">_builder_register_method_docs</span><span class="p">(</span><span class="n">original_name</span><span class="p">,</span> <span class="n">library_path</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">fn_signature</span><span class="p">,</span> <span class="n">fn_docs</span><span class="p">)</span>

        <span class="nb">setattr</span><span class="p">(</span><span class="n">cls</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">fn</span><span class="p">)</span>
        <span class="c1">#exec(&quot;Builder.{0} = fn&quot;.format(name))</span>


    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">register_map_method</span><span class="p">(</span><span class="n">cls</span><span class="p">,</span> <span class="n">fn</span><span class="p">,</span> <span class="n">library_path</span><span class="p">,</span> <span class="n">alias</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">doc</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This method enables you to register any function `fn` that takes a Tensor as its first argument and returns a Tensor as a method of the Builder class. The resulting method is created by *lifting* the function to work with a Builder.</span>

<span class="sd">        **Arguments**</span>

<span class="sd">        * `fn`: a function of type `Tensor -&gt; Tensor`.</span>
<span class="sd">        * `library_path`: the route of the librar from which this function was taken, used for documentation purposes.</span>
<span class="sd">        * `alias`: allows you to specify the name of the method, it will take the name of the function if its `None`.</span>
<span class="sd">        * `doc`: the documentation for the method, if `None` a predefied documentation will be generated based on the documentation of `fn`.</span>

<span class="sd">        **Return**</span>

<span class="sd">        `None`</span>

<span class="sd">        **Examples**</span>

<span class="sd">        In this example we will register `tf.reshape` as a method of the Builder class</span>

<span class="sd">            import tensorflow as tf</span>
<span class="sd">            from tensorbuilder import tb</span>

<span class="sd">            tb.Builder.register_map_method(tf.reshape, &quot;tf&quot;)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">fn_signature</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">get_method_sig</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span>
     	<span class="n">fn_docs</span> <span class="o">=</span> <span class="n">inspect</span><span class="o">.</span><span class="n">getdoc</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span>
        <span class="n">original_name</span> <span class="o">=</span> <span class="n">fn</span><span class="o">.</span><span class="n">__name__</span>
        <span class="n">name</span> <span class="o">=</span> <span class="n">alias</span> <span class="k">if</span> <span class="n">alias</span> <span class="k">else</span> <span class="n">original_name</span>

        <span class="n">lifted</span> <span class="o">=</span> <span class="n">_lift</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span>
        <span class="n">lifted</span><span class="o">.</span><span class="n">__name__</span> <span class="o">=</span> <span class="n">name</span>
        <span class="n">lifted</span><span class="o">.</span><span class="n">__doc__</span> <span class="o">=</span> <span class="n">doc</span> <span class="k">if</span> <span class="n">doc</span> <span class="k">else</span> <span class="n">_builder_register_map_method_docs</span><span class="p">(</span><span class="n">original_name</span><span class="p">,</span> <span class="n">library_path</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">fn_signature</span><span class="p">,</span> <span class="n">fn_docs</span><span class="p">)</span>

        <span class="nb">setattr</span><span class="p">(</span><span class="n">cls</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">lifted</span><span class="p">)</span>


    <span class="nd">@immutable</span>
    <span class="k">def</span> <span class="nf">map</span><span class="p">(</span><span class="n">builder</span><span class="p">,</span> <span class="n">fn</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        `@immutable`</span>

<span class="sd">        Let **x** be Tensor inside a Builder `builder` and **fn** be a function from a tensor to a tensor, then `builder.map(fn, \*args, **kwargs)` computes `fn(x, *args, **kwargs) and stores the result inside a Builder`. The Builder class comes with a lot of **patched** methods that help you do things quickly and make the syntax nicer, but if we don&#39;t have the method you need just pass the function you want to use to `map`, or even consider using `tensorbuilder.core.builders.Builder.register_map_method`.</span>

<span class="sd">        **Parameters**</span>

<span class="sd">        * `fn`: a function of type `tensor -&gt; tensor`.</span>
<span class="sd">        * All extra positional and named arguments are forwarded to `fn`</span>

<span class="sd">        **Return**</span>

<span class="sd">        * `tensorbuilder.core.builders.Builder`</span>

<span class="sd">        **Examples**</span>

<span class="sd">            import tensorflow as tf</span>
<span class="sd">            from tensorflow.contrib import layers</span>
<span class="sd">            from tensorbuilder import tb</span>

<span class="sd">            x = tf.placeholder(tf.float32, shape=[None, 40])</span>
<span class="sd">            keep_prob = tf.placeholder(tf.float32)</span>

<span class="sd">            h = (</span>
<span class="sd">            	tb.build(x)</span>
<span class="sd">            	.map(layers.fully_connected, 100, activation_fn=tf.nn.tanh)</span>
<span class="sd">            	.map(tf.nn.dropout, keep_prob)</span>
<span class="sd">            	.map(layers.fully_connected, 30, activation_fn=tf.nn.softmax)</span>
<span class="sd">            	.tensor()</span>
<span class="sd">            )</span>

<span class="sd">            print(h)</span>

<span class="sd">        Same using the DSL</span>

<span class="sd">            import tensorflow as tf</span>
<span class="sd">            from tensorflow.contrib import layers</span>
<span class="sd">            from tensorbuilder import tb</span>


<span class="sd">            x = tf.placeholder(tf.float32, shape=[None, 40])</span>
<span class="sd">            keep_prob = tf.placeholder(tf.float32)</span>

<span class="sd">            h = tb.pipe(</span>
<span class="sd">            	x,</span>
<span class="sd">            	tb.map(layers.fully_connected, 100, activation_fn=tf.nn.tanh)</span>
<span class="sd">            	.map(tf.nn.dropout, keep_prob)</span>
<span class="sd">            	.map(layers.fully_connected, 30, activation_fn=tf.nn.softmax)</span>
<span class="sd">            	.tensor()</span>
<span class="sd">            )</span>

<span class="sd">            print(h)</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">tensor</span> <span class="o">=</span> <span class="n">fn</span><span class="p">(</span><span class="n">builder</span><span class="o">.</span><span class="n">tensor</span><span class="p">(),</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">builder</span><span class="o">.</span><span class="n">_unit</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>

    <span class="nd">@immutable</span>
    <span class="k">def</span> <span class="nf">then</span><span class="p">(</span><span class="n">builder</span><span class="p">,</span> <span class="n">fn</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        `@immutable`</span>

<span class="sd">        Expects a function **fn** with type `builder -&gt; builder`. This method is used primarily to manupilate the Builder with very fine grain control through the fluent immutable API.</span>

<span class="sd">        **Parameters**</span>

<span class="sd">        * `fn`: a function of type `builder -&gt; builder`.</span>

<span class="sd">        **Return**</span>

<span class="sd">        * `tensorbuilder.core.builders.Builder`</span>

<span class="sd">        ** Example **</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">fn</span><span class="p">(</span><span class="n">builder</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="nd">@immutable</span>
    <span class="k">def</span> <span class="nf">branch</span><span class="p">(</span><span class="n">builder</span><span class="p">,</span> <span class="n">fn</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        `@immutable`</span>

<span class="sd">        Expects a function **fn** with type `Builder -&gt; iterable( Builder | BuilderTree )`. This method enables you to *branch* the computational graph so you can easily create neural networks with more complex topologies.</span>

<span class="sd">        **Parameters**</span>

<span class="sd">        * `fn`: a function of type `Builder -&gt; iterable( Builder | BuilderTree )`.</span>

<span class="sd">        **Return**</span>

<span class="sd">        * `tensorbuilder.core.builders.BuilderTree`</span>

<span class="sd">        ** Examples **</span>

<span class="sd">            import tensorflow as tf</span>
<span class="sd">            from tensorbuilder import tb</span>

<span class="sd">            x = placeholder(tf.float32, shape=[None, 10])</span>

<span class="sd">            h = (</span>
<span class="sd">                tb.build(x)</span>
<span class="sd">                .branch(lambda x: [</span>
<span class="sd">                    x.relu_layer(20)</span>
<span class="sd">                ,</span>
<span class="sd">                    x.sigmoid_layer(20)</span>
<span class="sd">                ,</span>
<span class="sd">                    x.tanh_layer(20)</span>
<span class="sd">                ])</span>
<span class="sd">                .softmax_layer(5)</span>
<span class="sd">                .tensor()</span>
<span class="sd">            )</span>

<span class="sd">        Same with the DSL</span>

<span class="sd">            import tensorflow as tf</span>
<span class="sd">            from tensorbuilder import tb</span>

<span class="sd">            x = placeholder(tf.float32, shape=[None, 10])</span>

<span class="sd">            h = tb.pipe(</span>
<span class="sd">                x,</span>
<span class="sd">                [</span>
<span class="sd">                    tb.relu_layer(20)</span>
<span class="sd">                ,</span>
<span class="sd">                    tb.sigmoid_layer(20)</span>
<span class="sd">                ,</span>
<span class="sd">                    tb.tanh_layer(20)</span>
<span class="sd">                ],</span>
<span class="sd">                tb.softmax_layer(5)</span>
<span class="sd">                .tensor()</span>
<span class="sd">            )</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">builder</span><span class="o">.</span><span class="n">BuilderTree</span><span class="p">(</span><span class="n">fn</span><span class="p">(</span><span class="n">builder</span><span class="p">))</span>

    <span class="nd">@immutable</span>
    <span class="k">def</span> <span class="nf">then_with</span><span class="p">(</span><span class="n">builder</span><span class="p">,</span> <span class="n">scope_fn</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        `@immutable`</span>

<span class="sd">        Expects a function **fn** with that returns a &quot;Disposable&quot; (implement `__enter__` and `__exit__`) plus some \*args and \*\*kwargs, and return a function `g` that expects a function `h` of type `Builder -&gt; Builder` such that</span>

<span class="sd">            .then_with(fn, *args, **kwargs)(h)</span>

<span class="sd">        roughly perform this computations (given the current `builder`)</span>

<span class="sd">            with fn(*args, **kwargs):</span>
<span class="sd">                return h(builder)</span>

<span class="sd">        For a more practical understanding look at the example.</span>

<span class="sd">        **Parameters**</span>

<span class="sd">        * `fn`: a function of type `Builder -&gt; Disposable`.</span>

<span class="sd">        **Return**</span>

<span class="sd">        * Function of type `(Builder -&gt; Builder)`</span>

<span class="sd">        ** Examples **</span>

<span class="sd">        Create a network with 3 branches and execute each on the devices &quot;/gpu:0&quot;, &quot;/gpu:1&quot;, &quot;cpu:3&quot; respectively</span>

<span class="sd">            import tensorflow as tf</span>
<span class="sd">            from tensorbuilder import tb</span>

<span class="sd">            x = placeholder(tf.float32, shape=[None, 10])</span>

<span class="sd">            h = (</span>
<span class="sd">                tb.build(x)</span>
<span class="sd">                .branch(lambda x: [</span>
<span class="sd">                    x.then_with(tf.device, &quot;/gpu:0&quot;)(lambda x:</span>
<span class="sd">                        x.relu_layer(20)</span>
<span class="sd">                        .linear_layer(5)</span>
<span class="sd">                    )</span>
<span class="sd">                ,</span>
<span class="sd">                    x.then_with(tf.device, &quot;/gpu:1&quot;)(lambda x:</span>
<span class="sd">                        x.sigmoid_layer(20)</span>
<span class="sd">                        .linear_layer(5)</span>
<span class="sd">                    )</span>
<span class="sd">                ,</span>
<span class="sd">                    x.then_with(tf.device, &quot;/cpu:0&quot;)(lambda x:</span>
<span class="sd">                        x.tanh_layer(20)</span>
<span class="sd">                        .linear_layer(5)</span>
<span class="sd">                    )</span>
<span class="sd">                ])</span>
<span class="sd">                .reduce(tf.add)</span>
<span class="sd">                .softmax()</span>
<span class="sd">                .tensor()</span>
<span class="sd">            )</span>

<span class="sd">        This looks much better with the DSL thanks to its support for scopes</span>

<span class="sd">            import tensorflow as tf</span>
<span class="sd">            from tensorbuilder import tb</span>

<span class="sd">            x = placeholder(tf.float32, shape=[None, 10])</span>

<span class="sd">            h = tb.pipe(</span>
<span class="sd">                x,</span>
<span class="sd">                [</span>
<span class="sd">                    { tf.device(&quot;/gpu:0&quot;):</span>
<span class="sd">                        tb.relu_layer(20)</span>
<span class="sd">                        .linear_layer(5)</span>
<span class="sd">                    }</span>
<span class="sd">                ,</span>
<span class="sd">                    { tf.device(&quot;/gpu:1&quot;):</span>
<span class="sd">                        tb.sigmoid_layer(20)</span>
<span class="sd">                        .linear_layer(5)</span>
<span class="sd">                    }</span>
<span class="sd">                ,</span>
<span class="sd">                    { tf.device(&quot;/cpu:0&quot;):</span>
<span class="sd">                        tb.tanh_layer(20)</span>
<span class="sd">                        .linear_layer(5)</span>
<span class="sd">                    }</span>
<span class="sd">                ],</span>
<span class="sd">                tb.reduce(tf.add)</span>
<span class="sd">                .softmax()</span>
<span class="sd">                .tensor()</span>
<span class="sd">            )</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">def</span> <span class="nf">_lambda</span><span class="p">(</span><span class="n">fn</span><span class="p">):</span>
            <span class="k">with</span> <span class="n">scope_fn</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
                <span class="n">y</span> <span class="o">=</span> <span class="n">fn</span><span class="p">(</span><span class="n">builder</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">y</span>
        <span class="k">return</span> <span class="n">_lambda</span>

    <span class="nd">@immutable</span>
    <span class="k">def</span> <span class="nf">__iter__</span><span class="p">(</span><span class="n">builder</span><span class="p">):</span>
        <span class="k">yield</span> <span class="n">builder</span>
</pre></div>

  </div>
</div>


      <div class="class">
          <h3>Ancestors (in MRO)</h3>
          <ul class="class_list">
          <li><a href="#tensorbuilder.core.BuilderBase">BuilderBase</a></li>
          <li>__builtin__.object</li>
          </ul>
          <h3>Methods</h3>
            
  <div class="item">
    <div class="name def" id="tensorbuilder.core.BuilderBase.__init__">
    <p>def <span class="ident">__init__</span>(</p><p>self, tensor)</p>
    </div>
    

    
  
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-tensorbuilder.core.BuilderBase.__init__', this);">Show source &equiv;</a></p>
  <div id="source-tensorbuilder.core.BuilderBase.__init__" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensor</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">BuilderBase</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_tensor</span> <span class="o">=</span> <span class="n">tensor</span>
    <span class="sd">&quot;&quot;&quot;A `tensorflow` Tensor.&quot;&quot;&quot;</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="tensorbuilder.core.BuilderBase.BuilderTree">
    <p>def <span class="ident">BuilderTree</span>(</p><p>self, builder_iterable)</p>
    </div>
    

    
  
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-tensorbuilder.core.BuilderBase.BuilderTree', this);">Show source &equiv;</a></p>
  <div id="source-tensorbuilder.core.BuilderBase.BuilderTree" class="source">
    <div class="codehilite"><pre><span></span><span class="nd">@abstractmethod</span>
<span class="k">def</span> <span class="nf">BuilderTree</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">builder_iterable</span><span class="p">):</span>
    <span class="k">pass</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="tensorbuilder.core.BuilderBase.branch">
    <p>def <span class="ident">branch</span>(</p><p>builder, fn)</p>
    </div>
    

    
  
    <div class="desc"><p><code>@immutable</code></p>
<p>Expects a function <strong>fn</strong> with type <code>Builder -&gt; iterable( Builder | BuilderTree )</code>. This method enables you to <em>branch</em> the computational graph so you can easily create neural networks with more complex topologies.</p>
<p><strong>Parameters</strong></p>
<ul>
<li><code>fn</code>: a function of type <code>Builder -&gt; iterable( Builder | BuilderTree )</code>.</li>
</ul>
<p><strong>Return</strong></p>
<ul>
<li><code>tensorbuilder.core.builders.BuilderTree</code></li>
</ul>
<p><strong> Examples </strong></p>
<div class="codehilite"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>
<span class="kn">from</span> <span class="nn">tensorbuilder</span> <span class="kn">import</span> <span class="n">tb</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">10</span><span class="p">])</span>

<span class="n">h</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">tb</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="o">.</span><span class="n">branch</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="p">[</span>
        <span class="n">x</span><span class="o">.</span><span class="n">relu_layer</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span>
    <span class="p">,</span>
        <span class="n">x</span><span class="o">.</span><span class="n">sigmoid_layer</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span>
    <span class="p">,</span>
        <span class="n">x</span><span class="o">.</span><span class="n">tanh_layer</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span>
    <span class="p">])</span>
    <span class="o">.</span><span class="n">softmax_layer</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
    <span class="o">.</span><span class="n">tensor</span><span class="p">()</span>
<span class="p">)</span>
</pre></div>


<p>Same with the DSL</p>
<div class="codehilite"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>
<span class="kn">from</span> <span class="nn">tensorbuilder</span> <span class="kn">import</span> <span class="n">tb</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">10</span><span class="p">])</span>

<span class="n">h</span> <span class="o">=</span> <span class="n">tb</span><span class="o">.</span><span class="n">pipe</span><span class="p">(</span>
    <span class="n">x</span><span class="p">,</span>
    <span class="p">[</span>
        <span class="n">tb</span><span class="o">.</span><span class="n">relu_layer</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span>
    <span class="p">,</span>
        <span class="n">tb</span><span class="o">.</span><span class="n">sigmoid_layer</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span>
    <span class="p">,</span>
        <span class="n">tb</span><span class="o">.</span><span class="n">tanh_layer</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span>
    <span class="p">],</span>
    <span class="n">tb</span><span class="o">.</span><span class="n">softmax_layer</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
    <span class="o">.</span><span class="n">tensor</span><span class="p">()</span>
<span class="p">)</span>
</pre></div></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-tensorbuilder.core.BuilderBase.branch', this);">Show source &equiv;</a></p>
  <div id="source-tensorbuilder.core.BuilderBase.branch" class="source">
    <div class="codehilite"><pre><span></span><span class="nd">@immutable</span>
<span class="k">def</span> <span class="nf">branch</span><span class="p">(</span><span class="n">builder</span><span class="p">,</span> <span class="n">fn</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    `@immutable`</span>
<span class="sd">    Expects a function **fn** with type `Builder -&gt; iterable( Builder | BuilderTree )`. This method enables you to *branch* the computational graph so you can easily create neural networks with more complex topologies.</span>
<span class="sd">    **Parameters**</span>
<span class="sd">    * `fn`: a function of type `Builder -&gt; iterable( Builder | BuilderTree )`.</span>
<span class="sd">    **Return**</span>
<span class="sd">    * `tensorbuilder.core.builders.BuilderTree`</span>
<span class="sd">    ** Examples **</span>
<span class="sd">        import tensorflow as tf</span>
<span class="sd">        from tensorbuilder import tb</span>
<span class="sd">        x = placeholder(tf.float32, shape=[None, 10])</span>
<span class="sd">        h = (</span>
<span class="sd">            tb.build(x)</span>
<span class="sd">            .branch(lambda x: [</span>
<span class="sd">                x.relu_layer(20)</span>
<span class="sd">            ,</span>
<span class="sd">                x.sigmoid_layer(20)</span>
<span class="sd">            ,</span>
<span class="sd">                x.tanh_layer(20)</span>
<span class="sd">            ])</span>
<span class="sd">            .softmax_layer(5)</span>
<span class="sd">            .tensor()</span>
<span class="sd">        )</span>
<span class="sd">    Same with the DSL</span>
<span class="sd">        import tensorflow as tf</span>
<span class="sd">        from tensorbuilder import tb</span>
<span class="sd">        x = placeholder(tf.float32, shape=[None, 10])</span>
<span class="sd">        h = tb.pipe(</span>
<span class="sd">            x,</span>
<span class="sd">            [</span>
<span class="sd">                tb.relu_layer(20)</span>
<span class="sd">            ,</span>
<span class="sd">                tb.sigmoid_layer(20)</span>
<span class="sd">            ,</span>
<span class="sd">                tb.tanh_layer(20)</span>
<span class="sd">            ],</span>
<span class="sd">            tb.softmax_layer(5)</span>
<span class="sd">            .tensor()</span>
<span class="sd">        )</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">builder</span><span class="o">.</span><span class="n">BuilderTree</span><span class="p">(</span><span class="n">fn</span><span class="p">(</span><span class="n">builder</span><span class="p">))</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="tensorbuilder.core.BuilderBase.copy">
    <p>def <span class="ident">copy</span>(</p><p>self)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns a copy of this Builder</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-tensorbuilder.core.BuilderBase.copy', this);">Show source &equiv;</a></p>
  <div id="source-tensorbuilder.core.BuilderBase.copy" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">copy</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns a copy of this Builder&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_unit</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tensor</span><span class="p">())</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="tensorbuilder.core.BuilderBase.map">
    <p>def <span class="ident">map</span>(</p><p>builder, fn, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p><code>@immutable</code></p>
<p>Let <strong>x</strong> be Tensor inside a Builder <code>builder</code> and <strong>fn</strong> be a function from a tensor to a tensor, then <code>builder.map(fn, \*args, **kwargs)</code> computes <code>fn(x, *args, **kwargs) and stores the result inside a Builder</code>. The Builder class comes with a lot of <strong>patched</strong> methods that help you do things quickly and make the syntax nicer, but if we don't have the method you need just pass the function you want to use to <code>map</code>, or even consider using <code>tensorbuilder.core.builders.Builder.register_map_method</code>.</p>
<p><strong>Parameters</strong></p>
<ul>
<li><code>fn</code>: a function of type <code>tensor -&gt; tensor</code>.</li>
<li>All extra positional and named arguments are forwarded to <code>fn</code></li>
</ul>
<p><strong>Return</strong></p>
<ul>
<li><code>tensorbuilder.core.builders.Builder</code></li>
</ul>
<p><strong>Examples</strong></p>
<div class="codehilite"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>
<span class="kn">from</span> <span class="nn">tensorflow.contrib</span> <span class="kn">import</span> <span class="n">layers</span>
<span class="kn">from</span> <span class="nn">tensorbuilder</span> <span class="kn">import</span> <span class="n">tb</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">40</span><span class="p">])</span>
<span class="n">keep_prob</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="n">h</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">tb</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">fully_connected</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="n">activation_fn</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">tanh</span><span class="p">)</span>
    <span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">dropout</span><span class="p">,</span> <span class="n">keep_prob</span><span class="p">)</span>
    <span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">fully_connected</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="n">activation_fn</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">)</span>
    <span class="o">.</span><span class="n">tensor</span><span class="p">()</span>
<span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
</pre></div>


<p>Same using the DSL</p>
<div class="codehilite"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>
<span class="kn">from</span> <span class="nn">tensorflow.contrib</span> <span class="kn">import</span> <span class="n">layers</span>
<span class="kn">from</span> <span class="nn">tensorbuilder</span> <span class="kn">import</span> <span class="n">tb</span>


<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">40</span><span class="p">])</span>
<span class="n">keep_prob</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="n">h</span> <span class="o">=</span> <span class="n">tb</span><span class="o">.</span><span class="n">pipe</span><span class="p">(</span>
    <span class="n">x</span><span class="p">,</span>
    <span class="n">tb</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">fully_connected</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="n">activation_fn</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">tanh</span><span class="p">)</span>
    <span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">dropout</span><span class="p">,</span> <span class="n">keep_prob</span><span class="p">)</span>
    <span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">fully_connected</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="n">activation_fn</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">)</span>
    <span class="o">.</span><span class="n">tensor</span><span class="p">()</span>
<span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
</pre></div></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-tensorbuilder.core.BuilderBase.map', this);">Show source &equiv;</a></p>
  <div id="source-tensorbuilder.core.BuilderBase.map" class="source">
    <div class="codehilite"><pre><span></span><span class="nd">@immutable</span>
<span class="k">def</span> <span class="nf">map</span><span class="p">(</span><span class="n">builder</span><span class="p">,</span> <span class="n">fn</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    `@immutable`</span>
<span class="sd">    Let **x** be Tensor inside a Builder `builder` and **fn** be a function from a tensor to a tensor, then `builder.map(fn, \*args, **kwargs)` computes `fn(x, *args, **kwargs) and stores the result inside a Builder`. The Builder class comes with a lot of **patched** methods that help you do things quickly and make the syntax nicer, but if we don&#39;t have the method you need just pass the function you want to use to `map`, or even consider using `tensorbuilder.core.builders.Builder.register_map_method`.</span>
<span class="sd">    **Parameters**</span>
<span class="sd">    * `fn`: a function of type `tensor -&gt; tensor`.</span>
<span class="sd">    * All extra positional and named arguments are forwarded to `fn`</span>
<span class="sd">    **Return**</span>
<span class="sd">    * `tensorbuilder.core.builders.Builder`</span>
<span class="sd">    **Examples**</span>
<span class="sd">        import tensorflow as tf</span>
<span class="sd">        from tensorflow.contrib import layers</span>
<span class="sd">        from tensorbuilder import tb</span>
<span class="sd">        x = tf.placeholder(tf.float32, shape=[None, 40])</span>
<span class="sd">        keep_prob = tf.placeholder(tf.float32)</span>
<span class="sd">        h = (</span>
<span class="sd">        	tb.build(x)</span>
<span class="sd">        	.map(layers.fully_connected, 100, activation_fn=tf.nn.tanh)</span>
<span class="sd">        	.map(tf.nn.dropout, keep_prob)</span>
<span class="sd">        	.map(layers.fully_connected, 30, activation_fn=tf.nn.softmax)</span>
<span class="sd">        	.tensor()</span>
<span class="sd">        )</span>
<span class="sd">        print(h)</span>
<span class="sd">    Same using the DSL</span>
<span class="sd">        import tensorflow as tf</span>
<span class="sd">        from tensorflow.contrib import layers</span>
<span class="sd">        from tensorbuilder import tb</span>
<span class="sd">        x = tf.placeholder(tf.float32, shape=[None, 40])</span>
<span class="sd">        keep_prob = tf.placeholder(tf.float32)</span>
<span class="sd">        h = tb.pipe(</span>
<span class="sd">        	x,</span>
<span class="sd">        	tb.map(layers.fully_connected, 100, activation_fn=tf.nn.tanh)</span>
<span class="sd">        	.map(tf.nn.dropout, keep_prob)</span>
<span class="sd">        	.map(layers.fully_connected, 30, activation_fn=tf.nn.softmax)</span>
<span class="sd">        	.tensor()</span>
<span class="sd">        )</span>
<span class="sd">        print(h)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">tensor</span> <span class="o">=</span> <span class="n">fn</span><span class="p">(</span><span class="n">builder</span><span class="o">.</span><span class="n">tensor</span><span class="p">(),</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">builder</span><span class="o">.</span><span class="n">_unit</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="tensorbuilder.core.BuilderBase.register_map_method">
    <p>def <span class="ident">register_map_method</span>(</p><p>cls, fn, library_path, alias=None, doc=None)</p>
    </div>
    

    
  
    <div class="desc"><p>This method enables you to register any function <code>fn</code> that takes a Tensor as its first argument and returns a Tensor as a method of the Builder class. The resulting method is created by <em>lifting</em> the function to work with a Builder.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>fn</code>: a function of type <code>Tensor -&gt; Tensor</code>.</li>
<li><code>library_path</code>: the route of the librar from which this function was taken, used for documentation purposes.</li>
<li><code>alias</code>: allows you to specify the name of the method, it will take the name of the function if its <code>None</code>.</li>
<li><code>doc</code>: the documentation for the method, if <code>None</code> a predefied documentation will be generated based on the documentation of <code>fn</code>.</li>
</ul>
<p><strong>Return</strong></p>
<p><code>None</code></p>
<p><strong>Examples</strong></p>
<p>In this example we will register <code>tf.reshape</code> as a method of the Builder class</p>
<div class="codehilite"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>
<span class="kn">from</span> <span class="nn">tensorbuilder</span> <span class="kn">import</span> <span class="n">tb</span>

<span class="n">tb</span><span class="o">.</span><span class="n">Builder</span><span class="o">.</span><span class="n">register_map_method</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">,</span> <span class="s2">&quot;tf&quot;</span><span class="p">)</span>
</pre></div></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-tensorbuilder.core.BuilderBase.register_map_method', this);">Show source &equiv;</a></p>
  <div id="source-tensorbuilder.core.BuilderBase.register_map_method" class="source">
    <div class="codehilite"><pre><span></span><span class="nd">@classmethod</span>
<span class="k">def</span> <span class="nf">register_map_method</span><span class="p">(</span><span class="n">cls</span><span class="p">,</span> <span class="n">fn</span><span class="p">,</span> <span class="n">library_path</span><span class="p">,</span> <span class="n">alias</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">doc</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This method enables you to register any function `fn` that takes a Tensor as its first argument and returns a Tensor as a method of the Builder class. The resulting method is created by *lifting* the function to work with a Builder.</span>
<span class="sd">    **Arguments**</span>
<span class="sd">    * `fn`: a function of type `Tensor -&gt; Tensor`.</span>
<span class="sd">    * `library_path`: the route of the librar from which this function was taken, used for documentation purposes.</span>
<span class="sd">    * `alias`: allows you to specify the name of the method, it will take the name of the function if its `None`.</span>
<span class="sd">    * `doc`: the documentation for the method, if `None` a predefied documentation will be generated based on the documentation of `fn`.</span>
<span class="sd">    **Return**</span>
<span class="sd">    `None`</span>
<span class="sd">    **Examples**</span>
<span class="sd">    In this example we will register `tf.reshape` as a method of the Builder class</span>
<span class="sd">        import tensorflow as tf</span>
<span class="sd">        from tensorbuilder import tb</span>
<span class="sd">        tb.Builder.register_map_method(tf.reshape, &quot;tf&quot;)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">fn_signature</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">get_method_sig</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span>
 	<span class="n">fn_docs</span> <span class="o">=</span> <span class="n">inspect</span><span class="o">.</span><span class="n">getdoc</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span>
    <span class="n">original_name</span> <span class="o">=</span> <span class="n">fn</span><span class="o">.</span><span class="n">__name__</span>
    <span class="n">name</span> <span class="o">=</span> <span class="n">alias</span> <span class="k">if</span> <span class="n">alias</span> <span class="k">else</span> <span class="n">original_name</span>
    <span class="n">lifted</span> <span class="o">=</span> <span class="n">_lift</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span>
    <span class="n">lifted</span><span class="o">.</span><span class="n">__name__</span> <span class="o">=</span> <span class="n">name</span>
    <span class="n">lifted</span><span class="o">.</span><span class="n">__doc__</span> <span class="o">=</span> <span class="n">doc</span> <span class="k">if</span> <span class="n">doc</span> <span class="k">else</span> <span class="n">_builder_register_map_method_docs</span><span class="p">(</span><span class="n">original_name</span><span class="p">,</span> <span class="n">library_path</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">fn_signature</span><span class="p">,</span> <span class="n">fn_docs</span><span class="p">)</span>
    <span class="nb">setattr</span><span class="p">(</span><span class="n">cls</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">lifted</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="tensorbuilder.core.BuilderBase.register_method">
    <p>def <span class="ident">register_method</span>(</p><p>cls, fn, library_path, alias=None, doc=None)</p>
    </div>
    

    
  
    <div class="desc"><p>This method enables you to register any function <code>fn</code> that takes a Builder as its first argument as a method of the Builder class.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>fn</code>: a function that atleast takes a Builder as its first argument.</li>
<li><code>library_path</code>: the route of the librar from which this function was taken, used for documentation purposes.</li>
<li><code>alias</code>: allows you to specify the name of the method, it will take the name of the function if its <code>None</code>.</li>
<li><code>doc</code>: the documentation for the method, if <code>None</code> a predefied documentation will be generated based on the documentation of <code>fn</code>.</li>
</ul>
<p><strong>Return</strong></p>
<p><code>None</code></p>
<p><strong>Examples</strong></p>
<p>In this example we will create a funtion and register it as a method called <code>relu_dropout_layer</code></p>
<div class="codehilite"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>
<span class="kn">from</span> <span class="nn">tensorbuilder</span> <span class="kn">import</span> <span class="n">tb</span>


<span class="k">def</span> <span class="nf">relu_dropout</span><span class="p">(</span><span class="n">builder</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">keep_prob</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Fully connect to a relu layer of size `size` and apply dropout with `keep_prob`&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">(</span>
        <span class="n">builder</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">fully_connected</span><span class="p">,</span> <span class="n">size</span><span class="p">)</span>
        <span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">)</span>
        <span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">dropout</span><span class="p">,</span> <span class="n">keep_prob</span><span class="p">)</span>
    <span class="p">)</span>

<span class="n">tb</span><span class="o">.</span><span class="n">Builder</span><span class="o">.</span><span class="n">register_method</span><span class="p">(</span><span class="n">relu_dropout_layer</span><span class="p">,</span> <span class="s2">&quot;my.lib&quot;</span><span class="p">,</span> <span class="n">alias</span><span class="o">=</span><span class="s2">&quot;relu_dropout_layer&quot;</span><span class="p">)</span>
</pre></div></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-tensorbuilder.core.BuilderBase.register_method', this);">Show source &equiv;</a></p>
  <div id="source-tensorbuilder.core.BuilderBase.register_method" class="source">
    <div class="codehilite"><pre><span></span><span class="nd">@classmethod</span>
<span class="k">def</span> <span class="nf">register_method</span><span class="p">(</span><span class="n">cls</span><span class="p">,</span> <span class="n">fn</span><span class="p">,</span> <span class="n">library_path</span><span class="p">,</span> <span class="n">alias</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">doc</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This method enables you to register any function `fn` that takes a Builder as its first argument as a method of the Builder class.</span>
<span class="sd">    **Arguments**</span>
<span class="sd">    * `fn`: a function that atleast takes a Builder as its first argument.</span>
<span class="sd">    * `library_path`: the route of the librar from which this function was taken, used for documentation purposes.</span>
<span class="sd">    * `alias`: allows you to specify the name of the method, it will take the name of the function if its `None`.</span>
<span class="sd">    * `doc`: the documentation for the method, if `None` a predefied documentation will be generated based on the documentation of `fn`.</span>
<span class="sd">    **Return**</span>
<span class="sd">    `None`</span>
<span class="sd">    **Examples**</span>
<span class="sd">    In this example we will create a funtion and register it as a method called `relu_dropout_layer`</span>
<span class="sd">        import tensorflow as tf</span>
<span class="sd">        from tensorbuilder import tb</span>
<span class="sd">        def relu_dropout(builder, size, keep_prob):</span>
<span class="sd">            \&quot;\&quot;\&quot;Fully connect to a relu layer of size `size` and apply dropout with `keep_prob`\&quot;\&quot;\&quot;</span>
<span class="sd">            return (</span>
<span class="sd">                builder.map(tf.contrib.layers.fully_connected, size)</span>
<span class="sd">                .map(tf.nn.relu)</span>
<span class="sd">                .map(tf.nn.dropout, keep_prob)</span>
<span class="sd">            )</span>
<span class="sd">        tb.Builder.register_method(relu_dropout_layer, &quot;my.lib&quot;, alias=&quot;relu_dropout_layer&quot;)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">fn_signature</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">get_method_sig</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span>
 	<span class="n">fn_docs</span> <span class="o">=</span> <span class="n">inspect</span><span class="o">.</span><span class="n">getdoc</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span>
    <span class="n">original_name</span> <span class="o">=</span> <span class="n">fn</span><span class="o">.</span><span class="n">__name__</span>
    <span class="n">name</span> <span class="o">=</span> <span class="n">alias</span> <span class="k">if</span> <span class="n">alias</span> <span class="k">else</span> <span class="n">original_name</span>
    <span class="n">fn</span><span class="o">.</span><span class="n">__name__</span> <span class="o">=</span> <span class="n">name</span>
    <span class="n">fn</span><span class="o">.</span><span class="n">__doc__</span> <span class="o">=</span> <span class="n">doc</span> <span class="k">if</span> <span class="n">doc</span> <span class="k">else</span> <span class="n">_builder_register_method_docs</span><span class="p">(</span><span class="n">original_name</span><span class="p">,</span> <span class="n">library_path</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">fn_signature</span><span class="p">,</span> <span class="n">fn_docs</span><span class="p">)</span>
    <span class="nb">setattr</span><span class="p">(</span><span class="n">cls</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">fn</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="tensorbuilder.core.BuilderBase.store_on">
    <p>def <span class="ident">store_on</span>(</p><p>builder, other)</p>
    </div>
    

    
  
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-tensorbuilder.core.BuilderBase.store_on', this);">Show source &equiv;</a></p>
  <div id="source-tensorbuilder.core.BuilderBase.store_on" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">store_on</span><span class="p">(</span><span class="n">builder</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
    <span class="n">other</span><span class="o">.</span><span class="n">_tensor</span> <span class="o">=</span> <span class="n">builder</span><span class="o">.</span><span class="n">tensor</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">builder</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="tensorbuilder.core.BuilderBase.tensor">
    <p>def <span class="ident">tensor</span>(</p><p>self)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns the Tensor contianed by the Builder</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-tensorbuilder.core.BuilderBase.tensor', this);">Show source &equiv;</a></p>
  <div id="source-tensorbuilder.core.BuilderBase.tensor" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">tensor</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="s2">&quot;Returns the Tensor contianed by the Builder&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tensor</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="tensorbuilder.core.BuilderBase.then">
    <p>def <span class="ident">then</span>(</p><p>builder, fn, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p><code>@immutable</code></p>
<p>Expects a function <strong>fn</strong> with type <code>builder -&gt; builder</code>. This method is used primarily to manupilate the Builder with very fine grain control through the fluent immutable API.</p>
<p><strong>Parameters</strong></p>
<ul>
<li><code>fn</code>: a function of type <code>builder -&gt; builder</code>.</li>
</ul>
<p><strong>Return</strong></p>
<ul>
<li><code>tensorbuilder.core.builders.Builder</code></li>
</ul>
<p><strong> Example </strong></p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-tensorbuilder.core.BuilderBase.then', this);">Show source &equiv;</a></p>
  <div id="source-tensorbuilder.core.BuilderBase.then" class="source">
    <div class="codehilite"><pre><span></span><span class="nd">@immutable</span>
<span class="k">def</span> <span class="nf">then</span><span class="p">(</span><span class="n">builder</span><span class="p">,</span> <span class="n">fn</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    `@immutable`</span>
<span class="sd">    Expects a function **fn** with type `builder -&gt; builder`. This method is used primarily to manupilate the Builder with very fine grain control through the fluent immutable API.</span>
<span class="sd">    **Parameters**</span>
<span class="sd">    * `fn`: a function of type `builder -&gt; builder`.</span>
<span class="sd">    **Return**</span>
<span class="sd">    * `tensorbuilder.core.builders.Builder`</span>
<span class="sd">    ** Example **</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">fn</span><span class="p">(</span><span class="n">builder</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="tensorbuilder.core.BuilderBase.then_with">
    <p>def <span class="ident">then_with</span>(</p><p>builder, scope_fn, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p><code>@immutable</code></p>
<p>Expects a function <strong>fn</strong> with that returns a "Disposable" (implement <code>__enter__</code> and <code>__exit__</code>) plus some *args and **kwargs, and return a function <code>g</code> that expects a function <code>h</code> of type <code>Builder -&gt; Builder</code> such that</p>
<div class="codehilite"><pre><span></span>.then_with(fn, *args, **kwargs)(h)
</pre></div>


<p>roughly perform this computations (given the current <code>builder</code>)</p>
<div class="codehilite"><pre><span></span>with fn(*args, **kwargs):
    return h(builder)
</pre></div>


<p>For a more practical understanding look at the example.</p>
<p><strong>Parameters</strong></p>
<ul>
<li><code>fn</code>: a function of type <code>Builder -&gt; Disposable</code>.</li>
</ul>
<p><strong>Return</strong></p>
<ul>
<li>Function of type <code>(Builder -&gt; Builder)</code></li>
</ul>
<p><strong> Examples </strong></p>
<p>Create a network with 3 branches and execute each on the devices "/gpu:0", "/gpu:1", "cpu:3" respectively</p>
<div class="codehilite"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>
<span class="kn">from</span> <span class="nn">tensorbuilder</span> <span class="kn">import</span> <span class="n">tb</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">10</span><span class="p">])</span>

<span class="n">h</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">tb</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="o">.</span><span class="n">branch</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="p">[</span>
        <span class="n">x</span><span class="o">.</span><span class="n">then_with</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="s2">&quot;/gpu:0&quot;</span><span class="p">)(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span>
            <span class="n">x</span><span class="o">.</span><span class="n">relu_layer</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span>
            <span class="o">.</span><span class="n">linear_layer</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
        <span class="p">)</span>
    <span class="p">,</span>
        <span class="n">x</span><span class="o">.</span><span class="n">then_with</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="s2">&quot;/gpu:1&quot;</span><span class="p">)(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span>
            <span class="n">x</span><span class="o">.</span><span class="n">sigmoid_layer</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span>
            <span class="o">.</span><span class="n">linear_layer</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
        <span class="p">)</span>
    <span class="p">,</span>
        <span class="n">x</span><span class="o">.</span><span class="n">then_with</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="s2">&quot;/cpu:0&quot;</span><span class="p">)(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span>
            <span class="n">x</span><span class="o">.</span><span class="n">tanh_layer</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span>
            <span class="o">.</span><span class="n">linear_layer</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
        <span class="p">)</span>
    <span class="p">])</span>
    <span class="o">.</span><span class="n">reduce</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">add</span><span class="p">)</span>
    <span class="o">.</span><span class="n">softmax</span><span class="p">()</span>
    <span class="o">.</span><span class="n">tensor</span><span class="p">()</span>
<span class="p">)</span>
</pre></div>


<p>This looks much better with the DSL thanks to its support for scopes</p>
<div class="codehilite"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>
<span class="kn">from</span> <span class="nn">tensorbuilder</span> <span class="kn">import</span> <span class="n">tb</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">10</span><span class="p">])</span>

<span class="n">h</span> <span class="o">=</span> <span class="n">tb</span><span class="o">.</span><span class="n">pipe</span><span class="p">(</span>
    <span class="n">x</span><span class="p">,</span>
    <span class="p">[</span>
        <span class="p">{</span> <span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;/gpu:0&quot;</span><span class="p">):</span>
            <span class="n">tb</span><span class="o">.</span><span class="n">relu_layer</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span>
            <span class="o">.</span><span class="n">linear_layer</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
        <span class="p">}</span>
    <span class="p">,</span>
        <span class="p">{</span> <span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;/gpu:1&quot;</span><span class="p">):</span>
            <span class="n">tb</span><span class="o">.</span><span class="n">sigmoid_layer</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span>
            <span class="o">.</span><span class="n">linear_layer</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
        <span class="p">}</span>
    <span class="p">,</span>
        <span class="p">{</span> <span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;/cpu:0&quot;</span><span class="p">):</span>
            <span class="n">tb</span><span class="o">.</span><span class="n">tanh_layer</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span>
            <span class="o">.</span><span class="n">linear_layer</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
        <span class="p">}</span>
    <span class="p">],</span>
    <span class="n">tb</span><span class="o">.</span><span class="n">reduce</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">add</span><span class="p">)</span>
    <span class="o">.</span><span class="n">softmax</span><span class="p">()</span>
    <span class="o">.</span><span class="n">tensor</span><span class="p">()</span>
<span class="p">)</span>
</pre></div></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-tensorbuilder.core.BuilderBase.then_with', this);">Show source &equiv;</a></p>
  <div id="source-tensorbuilder.core.BuilderBase.then_with" class="source">
    <div class="codehilite"><pre><span></span><span class="nd">@immutable</span>
<span class="k">def</span> <span class="nf">then_with</span><span class="p">(</span><span class="n">builder</span><span class="p">,</span> <span class="n">scope_fn</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    `@immutable`</span>
<span class="sd">    Expects a function **fn** with that returns a &quot;Disposable&quot; (implement `__enter__` and `__exit__`) plus some \*args and \*\*kwargs, and return a function `g` that expects a function `h` of type `Builder -&gt; Builder` such that</span>
<span class="sd">        .then_with(fn, *args, **kwargs)(h)</span>
<span class="sd">    roughly perform this computations (given the current `builder`)</span>
<span class="sd">        with fn(*args, **kwargs):</span>
<span class="sd">            return h(builder)</span>
<span class="sd">    For a more practical understanding look at the example.</span>
<span class="sd">    **Parameters**</span>
<span class="sd">    * `fn`: a function of type `Builder -&gt; Disposable`.</span>
<span class="sd">    **Return**</span>
<span class="sd">    * Function of type `(Builder -&gt; Builder)`</span>
<span class="sd">    ** Examples **</span>
<span class="sd">    Create a network with 3 branches and execute each on the devices &quot;/gpu:0&quot;, &quot;/gpu:1&quot;, &quot;cpu:3&quot; respectively</span>
<span class="sd">        import tensorflow as tf</span>
<span class="sd">        from tensorbuilder import tb</span>
<span class="sd">        x = placeholder(tf.float32, shape=[None, 10])</span>
<span class="sd">        h = (</span>
<span class="sd">            tb.build(x)</span>
<span class="sd">            .branch(lambda x: [</span>
<span class="sd">                x.then_with(tf.device, &quot;/gpu:0&quot;)(lambda x:</span>
<span class="sd">                    x.relu_layer(20)</span>
<span class="sd">                    .linear_layer(5)</span>
<span class="sd">                )</span>
<span class="sd">            ,</span>
<span class="sd">                x.then_with(tf.device, &quot;/gpu:1&quot;)(lambda x:</span>
<span class="sd">                    x.sigmoid_layer(20)</span>
<span class="sd">                    .linear_layer(5)</span>
<span class="sd">                )</span>
<span class="sd">            ,</span>
<span class="sd">                x.then_with(tf.device, &quot;/cpu:0&quot;)(lambda x:</span>
<span class="sd">                    x.tanh_layer(20)</span>
<span class="sd">                    .linear_layer(5)</span>
<span class="sd">                )</span>
<span class="sd">            ])</span>
<span class="sd">            .reduce(tf.add)</span>
<span class="sd">            .softmax()</span>
<span class="sd">            .tensor()</span>
<span class="sd">        )</span>
<span class="sd">    This looks much better with the DSL thanks to its support for scopes</span>
<span class="sd">        import tensorflow as tf</span>
<span class="sd">        from tensorbuilder import tb</span>
<span class="sd">        x = placeholder(tf.float32, shape=[None, 10])</span>
<span class="sd">        h = tb.pipe(</span>
<span class="sd">            x,</span>
<span class="sd">            [</span>
<span class="sd">                { tf.device(&quot;/gpu:0&quot;):</span>
<span class="sd">                    tb.relu_layer(20)</span>
<span class="sd">                    .linear_layer(5)</span>
<span class="sd">                }</span>
<span class="sd">            ,</span>
<span class="sd">                { tf.device(&quot;/gpu:1&quot;):</span>
<span class="sd">                    tb.sigmoid_layer(20)</span>
<span class="sd">                    .linear_layer(5)</span>
<span class="sd">                }</span>
<span class="sd">            ,</span>
<span class="sd">                { tf.device(&quot;/cpu:0&quot;):</span>
<span class="sd">                    tb.tanh_layer(20)</span>
<span class="sd">                    .linear_layer(5)</span>
<span class="sd">                }</span>
<span class="sd">            ],</span>
<span class="sd">            tb.reduce(tf.add)</span>
<span class="sd">            .softmax()</span>
<span class="sd">            .tensor()</span>
<span class="sd">        )</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">_lambda</span><span class="p">(</span><span class="n">fn</span><span class="p">):</span>
        <span class="k">with</span> <span class="n">scope_fn</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">fn</span><span class="p">(</span><span class="n">builder</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">y</span>
    <span class="k">return</span> <span class="n">_lambda</span>
</pre></div>

  </div>
</div>

  </div>
  
      </div>
      </div>
      
      <div class="item">
      <p id="tensorbuilder.core.BuilderTreeBase" class="name">class <span class="ident">BuilderTreeBase</span></p>
      
  
    <div class="desc"><p>BuilderTree is a class that enables you to perform computations over a complex branched builder. It contains methods to handle the leaf <code>tensorbuilder.core.builders.Builder</code> nodes.</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-tensorbuilder.core.BuilderTreeBase', this);">Show source &equiv;</a></p>
  <div id="source-tensorbuilder.core.BuilderTreeBase" class="source">
    <div class="codehilite"><pre><span></span><span class="k">class</span> <span class="nc">BuilderTreeBase</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    BuilderTree is a class that enables you to perform computations over a complex branched builder. It contains methods to handle the leaf `tensorbuilder.core.builders.Builder` nodes.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__metaclass__</span> <span class="o">=</span> <span class="n">ABCMeta</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">builder_iterable</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">BuilderTreeBase</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_branches</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">builder_iterable</span><span class="p">)</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        An iterable that can contain elements that are of type `tensorbuilder.core.builders.Builder` or `tensorbuilder.core.builders.BuilderTree`.</span>
<span class="sd">        &quot;&quot;&quot;</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">Builder</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensor</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">copy</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_unit</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_branches</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_unit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">branches</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">__class__</span><span class="p">(</span><span class="n">branches</span><span class="p">)</span>



    <span class="nd">@immutable</span>
    <span class="k">def</span> <span class="nf">reduce</span><span class="p">(</span><span class="n">tree</span><span class="p">,</span> <span class="n">fn</span><span class="p">,</span> <span class="n">initializer</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        `@immutable`</span>

<span class="sd">        Expects a function **fn** with type `(Tensor, Tensor) -&gt; Tensor` and optionally an `initializer` and applies python [reduce](https://docs.python.org/2/library/functions.html#reduce) function to `tensorbuilder.core.builders.BuilderTree.tensors` using these arguments; the resulting Tensor is the wrapped inside a Builder.</span>

<span class="sd">        **Parameters**</span>

<span class="sd">        * `fn`: a function of type `(Tensor, Tensor) -&gt; Tensor`.</span>
<span class="sd">        * `initializer`: an optional Tensor as initial element of the folding operation (default: `None`)s</span>

<span class="sd">        **Return**</span>

<span class="sd">        * `tensorbuilder.core.builders.Builder`</span>

<span class="sd">        ** Example **</span>

<span class="sd">        Lets reduce the example on `tensorbuilder.core.builders.Builder.branch` this time doing the reduction ourselves instead of relying on the `*_layer` of `tensorbuilder.core.builders.BuilderTree` that do this for us</span>

<span class="sd">            import tensorflow as tf</span>
<span class="sd">            from tensorbuilder import tb</span>

<span class="sd">            x = placeholder(tf.float32, shape=[None, 10])</span>

<span class="sd">            h = (</span>
<span class="sd">                tb.build(x)</span>
<span class="sd">                .branch(lambda x: [</span>
<span class="sd">                    x.relu_layer(20)</span>
<span class="sd">                    .linear_layer(5)</span>
<span class="sd">                ,</span>
<span class="sd">                    x.sigmoid_layer(20)</span>
<span class="sd">                    .linear_layer(5)</span>
<span class="sd">                ,</span>
<span class="sd">                    x.tanh_layer(20)</span>
<span class="sd">                    .linear_layer(5)</span>
<span class="sd">                ])</span>
<span class="sd">                .reduce(tf.add)</span>
<span class="sd">                .softmax()</span>
<span class="sd">                .tensor()</span>
<span class="sd">            )</span>

<span class="sd">        Same example using the DSL</span>

<span class="sd">            import tensorflow as tf</span>
<span class="sd">            from tensorbuilder import tb</span>

<span class="sd">            x = placeholder(tf.float32, shape=[None, 10])</span>

<span class="sd">            h = tb.pipe(</span>
<span class="sd">                x,</span>
<span class="sd">                [</span>
<span class="sd">                    tb.relu_layer(20)</span>
<span class="sd">                    .linear_layer(5)</span>
<span class="sd">                ,</span>
<span class="sd">                    tb.sigmoid_layer(20)</span>
<span class="sd">                    .linear_layer(5)</span>
<span class="sd">                ,</span>
<span class="sd">                    tb.tanh_layer(20)</span>
<span class="sd">                    .linear_layer(5)</span>
<span class="sd">                ],</span>
<span class="sd">                tb.reduce(tf.add)</span>
<span class="sd">                .softmax()</span>
<span class="sd">                .tensor()</span>
<span class="sd">            )</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">initializer</span> <span class="o">!=</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">tensor</span> <span class="o">=</span> <span class="n">functools</span><span class="o">.</span><span class="n">reduce</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="n">tree</span><span class="o">.</span><span class="n">tensors</span><span class="p">(),</span> <span class="n">initializer</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">tensor</span> <span class="o">=</span> <span class="n">functools</span><span class="o">.</span><span class="n">reduce</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="n">tree</span><span class="o">.</span><span class="n">tensors</span><span class="p">())</span>

        <span class="k">return</span> <span class="n">tree</span><span class="o">.</span><span class="n">Builder</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>

    <span class="nd">@immutable</span>
    <span class="k">def</span> <span class="nf">map_each</span><span class="p">(</span><span class="n">tree</span><span class="p">,</span> <span class="n">fn</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        `@immutable`</span>

<span class="sd">        Expects a function **fn** with type `Tensor -&gt; Tensor` and applies this function to all leaf Tensors separately, resulting in a new BuilderTree.</span>

<span class="sd">        **Parameters**</span>

<span class="sd">        * `fn`: a function of type `Tensor -&gt; Tensor`.</span>
<span class="sd">        * All additional \*args and \*\*kwargs are forwarded to `fn`</span>

<span class="sd">        **Return**</span>

<span class="sd">        * `tensorbuilder.core.builders.BuilderTree`</span>

<span class="sd">        ** Example **</span>

<span class="sd">        Lets redu the example in `tensorbuilder.core.builders.BuilderTree.reduce` using `map_each` to reduce some code</span>

<span class="sd">            import tensorflow as tf</span>
<span class="sd">            from tensorbuilder import tb</span>

<span class="sd">            x = placeholder(tf.float32, shape=[None, 10])</span>

<span class="sd">            h = (</span>
<span class="sd">                tb.build(x)</span>
<span class="sd">                .branch(lambda x: [</span>
<span class="sd">                    x.relu_layer(20)</span>
<span class="sd">                ,</span>
<span class="sd">                    x.sigmoid_layer(20)</span>
<span class="sd">                ,</span>
<span class="sd">                    x.tanh_layer(20)</span>
<span class="sd">                ])</span>
<span class="sd">                .map_each(tf.contrib.layers.fully_connected, 5, activation_fn=None)</span>
<span class="sd">                .reduce(tf.add)</span>
<span class="sd">                .softmax()</span>
<span class="sd">                .tensor()</span>
<span class="sd">            )</span>

<span class="sd">        Remember that this</span>

<span class="sd">            .map_each(tf.contrib.layers.fully_connected, 5, activation_fn=None)</span>
<span class="sd">            .reduce(tf.add)</span>
<span class="sd">            .softmax()</span>

<span class="sd">        is equivalent to just</span>

<span class="sd">            .softmax_layer(5)</span>

<span class="sd">        for `BuilderTree`s. Same example using the DSL</span>

<span class="sd">            import tensorflow as tf</span>
<span class="sd">            from tensorbuilder import tb</span>

<span class="sd">            x = placeholder(tf.float32, shape=[None, 10])</span>

<span class="sd">            h = tb.pipe(</span>
<span class="sd">                x,</span>
<span class="sd">                [</span>
<span class="sd">                    x.relu_layer(20)</span>
<span class="sd">                ,</span>
<span class="sd">                    x.sigmoid_layer(20)</span>
<span class="sd">                ,</span>
<span class="sd">                    x.tanh_layer(20)</span>
<span class="sd">                ],</span>
<span class="sd">                tb.map_each(tf.contrib.layers.fully_connected, 5, activation_fn=None)</span>
<span class="sd">                .reduce(tf.add)</span>
<span class="sd">                .softmax()</span>
<span class="sd">                .tensor()</span>
<span class="sd">            )</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">branches</span> <span class="o">=</span> <span class="p">[</span> <span class="n">builder</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="k">for</span> <span class="n">builder</span> <span class="ow">in</span> <span class="n">tree</span> <span class="p">]</span>
        <span class="k">return</span> <span class="n">tree</span><span class="o">.</span><span class="n">_unit</span><span class="p">(</span><span class="n">branches</span><span class="p">)</span>

    <span class="nd">@immutable</span>
    <span class="k">def</span> <span class="nf">extract</span><span class="p">(</span><span class="n">tree</span><span class="p">,</span> <span class="n">fn</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        `@immutable`</span>

<span class="sd">        Expects a function **fn** with type `list( Tensor ) -&gt; Tensor` and applies this function to `tensorbuilder.core.builders.BuilderTree.tensors`, the resulting Tensor is wrapped in Builder. This function</span>

<span class="sd">        **Parameters**</span>

<span class="sd">        * `fn`: a function of type `list( Tensor ) -&gt; Tensor`.</span>
<span class="sd">        * All additional \*args and \*\*kwargs are forwarded to `fn`</span>

<span class="sd">        **Return**</span>

<span class="sd">        * `tensorbuilder.core.builders.Builder`</span>

<span class="sd">        ** Example **</span>

<span class="sd">        Lets redu the example in `tensorbuilder.core.builders.BuilderTree.map_each` using `extract`</span>

<span class="sd">            import tensorflow as tf</span>
<span class="sd">            from tensorbuilder import tb</span>

<span class="sd">            x = placeholder(tf.float32, shape=[None, 10])</span>

<span class="sd">            h = (</span>
<span class="sd">                tb.build(x)</span>
<span class="sd">                .branch(lambda x: [</span>
<span class="sd">                    x.relu_layer(20)</span>
<span class="sd">                ,</span>
<span class="sd">                    x.sigmoid_layer(20)</span>
<span class="sd">                ,</span>
<span class="sd">                    x.tanh_layer(20)</span>
<span class="sd">                ])</span>
<span class="sd">                .map_each(tf.contrib.layers.fully_connected, 5, activation_fn=None)</span>
<span class="sd">                .extract(lambda tensors: tf.add_n(tensors)) #or just .extract(tf.add_n)</span>
<span class="sd">                .softmax()</span>
<span class="sd">                .tensor()</span>
<span class="sd">            )</span>

<span class="sd">        Same example using the DSL</span>

<span class="sd">            import tensorflow as tf</span>
<span class="sd">            from tensorbuilder import tb</span>

<span class="sd">            x = placeholder(tf.float32, shape=[None, 10])</span>

<span class="sd">            h = (</span>
<span class="sd">                x,</span>
<span class="sd">                [</span>
<span class="sd">                    tb.relu_layer(20)</span>
<span class="sd">                ,</span>
<span class="sd">                    tb.sigmoid_layer(20)</span>
<span class="sd">                ,</span>
<span class="sd">                    tb.tanh_layer(20)</span>
<span class="sd">                ],</span>
<span class="sd">                tb.map_each(tf.contrib.layers.fully_connected, 5, activation_fn=None)</span>
<span class="sd">                .extract(lambda tensors: tf.add_n(tensors)) #or just .extract(tf.add_n)</span>
<span class="sd">                .softmax()</span>
<span class="sd">                .tensor()</span>
<span class="sd">            )</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">tensor</span> <span class="o">=</span> <span class="n">fn</span><span class="p">(</span><span class="n">tree</span><span class="o">.</span><span class="n">tensors</span><span class="p">(),</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">tree</span><span class="o">.</span><span class="n">Builder</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>



    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">register_method</span><span class="p">(</span><span class="n">cls</span><span class="p">,</span> <span class="n">fn</span><span class="p">,</span> <span class="n">library_path</span><span class="p">,</span> <span class="n">alias</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">doc</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This method enables you to register any function `fn` that takes a BuilderTree as its first argument as a method of the Builder class.</span>

<span class="sd">        **Arguments**</span>

<span class="sd">        * `fn`: a function that atleast takes a BuilderTree as its first argument.</span>
<span class="sd">        * `library_path`: the route of the librar from which this function was taken, used for documentation purposes.</span>
<span class="sd">        * `alias`: allows you to specify the name of the method, it will take the name of the function if its `None`.</span>
<span class="sd">        * `doc`: the documentation for the method, if `None` a predefied documentation will be generated based on the documentation of `fn`.</span>

<span class="sd">        **Return**</span>

<span class="sd">        `None`</span>

<span class="sd">        **Examples**</span>

<span class="sd">        In this example we will create the method `fully_connected` for the BuilderTree class</span>

<span class="sd">            import tensorflow as tf</span>
<span class="sd">            from tensorbuilder import tb</span>

<span class="sd">            def _tree_fully_connected(tree, size, *args, **kwargs):</span>
<span class="sd">                activation_fn = None</span>

<span class="sd">                if &quot;activation_fn&quot; in kwargs:</span>
<span class="sd">                    activation_fn = kwargs[&quot;activation_fn&quot;]</span>
<span class="sd">                    del kwargs[&quot;activation_fn&quot;]</span>

<span class="sd">                builder = (</span>
<span class="sd">                    tree.map_each(tf.contrib.layers.fully_connected, size, *args, **kwargs)</span>
<span class="sd">                    .reduce(tf.add)</span>
<span class="sd">                )</span>

<span class="sd">                if activation_fn:</span>
<span class="sd">                    builder = builder.map(activation_fn)</span>

<span class="sd">                return builder</span>

<span class="sd">            tb.BuilderTree.register_method(_tree_fully_connected, &quot;tensorbuilder.patches.tensorflow.fully_connected&quot;, alias=&quot;fully_connected&quot;)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">fn_signature</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">get_method_sig</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span>
     	<span class="n">fn_docs</span> <span class="o">=</span> <span class="n">inspect</span><span class="o">.</span><span class="n">getdoc</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span>
        <span class="n">original_name</span> <span class="o">=</span> <span class="n">fn</span><span class="o">.</span><span class="n">__name__</span>
        <span class="n">name</span> <span class="o">=</span> <span class="n">alias</span> <span class="k">if</span> <span class="n">alias</span> <span class="k">else</span> <span class="n">original_name</span>

        <span class="n">fn</span><span class="o">.</span><span class="n">__name__</span> <span class="o">=</span> <span class="n">name</span>
        <span class="n">fn</span><span class="o">.</span><span class="n">__doc__</span> <span class="o">=</span> <span class="n">doc</span> <span class="k">if</span> <span class="n">doc</span> <span class="k">else</span> <span class="n">_tree_register_method_docs</span><span class="p">(</span><span class="n">original_name</span><span class="p">,</span> <span class="n">library_path</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">fn_signature</span><span class="p">,</span> <span class="n">fn_docs</span><span class="p">)</span>

        <span class="nb">setattr</span><span class="p">(</span><span class="n">cls</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">fn</span><span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">register_reduce_method</span><span class="p">(</span><span class="n">cls</span><span class="p">,</span> <span class="n">fn</span><span class="p">,</span> <span class="n">library_path</span><span class="p">,</span> <span class="n">alias</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">doc</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This method enables you to register a function `fn` of type `(Tensor, Tensor) -&gt; Tensor` as a method of the Builder class.</span>

<span class="sd">        **Arguments**</span>

<span class="sd">        * `fn`: a function of type `(Tensor, Tensor) -&gt; Tensor`</span>
<span class="sd">        * `library_path`: the route of the librar from which this function was taken, used for documentation purposes.</span>
<span class="sd">        * `alias`: allows you to specify the name of the method, it will take the name of the function if its `None`.</span>
<span class="sd">        * `doc`: the documentation for the method, if `None` a predefied documentation will be generated based on the documentation of `fn`.</span>

<span class="sd">        **Return**</span>

<span class="sd">        `None`</span>

<span class="sd">        **Examples**</span>

<span class="sd">        In this example we will create the method `reduce_add` for the BuilderTree class</span>

<span class="sd">            import tensorflow as tf</span>
<span class="sd">            from tensorbuilder import tb</span>

<span class="sd">            tb.BuilderTree.register_reduce_method(tf.add, &quot;tf&quot;, alias=&quot;reduce_add&quot;)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">fn_signature</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">get_method_sig</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span>
     	<span class="n">fn_docs</span> <span class="o">=</span> <span class="n">inspect</span><span class="o">.</span><span class="n">getdoc</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span>
        <span class="n">original_name</span> <span class="o">=</span> <span class="n">fn</span><span class="o">.</span><span class="n">__name__</span>
        <span class="n">name</span> <span class="o">=</span> <span class="n">alias</span> <span class="k">if</span> <span class="n">alias</span> <span class="k">else</span> <span class="n">original_name</span>

        <span class="n">_tree_method</span> <span class="o">=</span> <span class="n">_lift_tree_reduce</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span>

        <span class="n">_tree_method</span><span class="o">.</span><span class="n">__name__</span> <span class="o">=</span> <span class="n">name</span>
        <span class="n">_tree_method</span><span class="o">.</span><span class="n">__doc__</span> <span class="o">=</span> <span class="n">doc</span> <span class="k">if</span> <span class="n">doc</span> <span class="k">else</span> <span class="n">_tree_register_reduce_method_docs</span><span class="p">(</span><span class="n">original_name</span><span class="p">,</span> <span class="n">library_path</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">fn_signature</span><span class="p">,</span> <span class="n">fn_docs</span><span class="p">)</span>

        <span class="nb">setattr</span><span class="p">(</span><span class="n">cls</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">_tree_method</span><span class="p">)</span>


    <span class="k">def</span> <span class="nf">builders</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns a flattened list `tensorbuilder.core.builders.Builder`s contained by this tree. The whole result is flattened in case of sub-elements are also `tensorbuilder.core.builders.BuilderTree`s.</span>

<span class="sd">        **Return**</span>

<span class="sd">        * `list( tensorbuilder.core.builders.Builder )`</span>

<span class="sd">        ** Examples **</span>

<span class="sd">        This examples creates a network to that solves the XOR problem using sigmoid units</span>

<span class="sd">            import tensorflow as tf</span>
<span class="sd">            from tensorbuilder import tb</span>

<span class="sd">            x = tf.placeholder(tf.float32, shape=[None, 2])</span>
<span class="sd">            y = tf.placeholder(tf.float32, shape=[None, 1])</span>


<span class="sd">            #Network</span>
<span class="sd">            [activation_builder, trainer_builder] = (</span>
<span class="sd">                tb.build(x)</span>

<span class="sd">                .sigmoid_layer(2)</span>
<span class="sd">                .linear_layer(1)</span>

<span class="sd">                .branch(lambda logit:</span>
<span class="sd">                [</span>
<span class="sd">                    logit.sigmoid() # activation</span>
<span class="sd">                ,</span>
<span class="sd">                    logit</span>
<span class="sd">                    .sigmoid_cross_entropy_with_logits(y) # loss</span>
<span class="sd">                    .map(tf.train.AdamOptimizer(0.01).minimize) # trainer</span>
<span class="sd">                ])</span>
<span class="sd">                .builders()</span>
<span class="sd">            )</span>

<span class="sd">        Same example using the DSL</span>

<span class="sd">            import tensorflow as tf</span>
<span class="sd">            from tensorbuilder import tb</span>

<span class="sd">            x = tf.placeholder(tf.float32, shape=[None, 2])</span>
<span class="sd">            y = tf.placeholder(tf.float32, shape=[None, 1])</span>


<span class="sd">            #Network</span>
<span class="sd">            [activation_builder, trainer_builder] = tb.pipe(</span>
<span class="sd">                x,</span>
<span class="sd">                tb.sigmoid_layer(2)</span>
<span class="sd">                .linear_layer(1),</span>
<span class="sd">                [</span>
<span class="sd">                    tb.sigmoid() # activation</span>
<span class="sd">                ,</span>
<span class="sd">                    tb</span>
<span class="sd">                    .sigmoid_cross_entropy_with_logits(y) # loss</span>
<span class="sd">                    .map(tf.train.AdamOptimizer(0.01).minimize) # trainer</span>
<span class="sd">                ],</span>
<span class="sd">                tb.builders()</span>
<span class="sd">            )</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="p">[</span> <span class="n">builder</span> <span class="k">for</span> <span class="n">builder</span> <span class="ow">in</span> <span class="bp">self</span> <span class="p">]</span>

    <span class="k">def</span> <span class="nf">tensors</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Same as `tensorbuilder.core.builders.BuilderTree.builders` but extracts the tensor from each `tensorbuilder.core.builders.Builder`.</span>

<span class="sd">        **Return**</span>

<span class="sd">        * `list( tf.Tensor )`</span>

<span class="sd">        ** Example **</span>

<span class="sd">        This examples creates a network to that solves the XOR problem using sigmoid units</span>

<span class="sd">            import tensorflow as tf</span>
<span class="sd">            from tensorbuilder import tb</span>

<span class="sd">            x = tf.placeholder(tf.float32, shape=[None, 2])</span>
<span class="sd">            y = tf.placeholder(tf.float32, shape=[None, 1])</span>


<span class="sd">            #Network</span>
<span class="sd">            [activation_tensor, trainer_tensor] = (</span>
<span class="sd">                tb.build(x)</span>

<span class="sd">                .sigmoid_layer(2)</span>
<span class="sd">                .linear_layer(1)</span>

<span class="sd">                .branch(lambda logit:</span>
<span class="sd">                [</span>
<span class="sd">                    logit.sigmoid() # activation</span>
<span class="sd">                ,</span>
<span class="sd">                    logit</span>
<span class="sd">                    .sigmoid_cross_entropy_with_logits(y) # loss</span>
<span class="sd">                    .map(tf.train.AdamOptimizer(0.01).minimize) # trainer</span>
<span class="sd">                ])</span>
<span class="sd">                .tensors()</span>
<span class="sd">            )</span>

<span class="sd">        Same example using the DSL</span>

<span class="sd">            import tensorflow as tf</span>
<span class="sd">            from tensorbuilder import tb</span>

<span class="sd">            x = tf.placeholder(tf.float32, shape=[None, 2])</span>
<span class="sd">            y = tf.placeholder(tf.float32, shape=[None, 1])</span>


<span class="sd">            #Network</span>
<span class="sd">            [activation_tensor, trainer_tensor] = tb.pipe(</span>
<span class="sd">                x,</span>
<span class="sd">                tb.sigmoid_layer(2)</span>
<span class="sd">                .linear_layer(1),</span>
<span class="sd">                [</span>
<span class="sd">                    tb.sigmoid() # activation</span>
<span class="sd">                ,</span>
<span class="sd">                    tb</span>
<span class="sd">                    .sigmoid_cross_entropy_with_logits(y) # loss</span>
<span class="sd">                    .map(tf.train.AdamOptimizer(0.01).minimize) # trainer</span>
<span class="sd">                ],</span>
<span class="sd">                tb.tensors()</span>
<span class="sd">            )</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="p">[</span> <span class="n">builder</span><span class="o">.</span><span class="n">_tensor</span> <span class="k">for</span> <span class="n">builder</span> <span class="ow">in</span> <span class="bp">self</span> <span class="p">]</span>


    <span class="nd">@immutable</span>
    <span class="k">def</span> <span class="nf">__iter__</span><span class="p">(</span><span class="n">tree</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;A generator function that lazily returns all the Builders contianed by this tree&quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">branch</span> <span class="ow">in</span> <span class="n">tree</span><span class="o">.</span><span class="n">_branches</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">builder</span> <span class="ow">in</span> <span class="n">branch</span><span class="p">:</span>
                <span class="k">yield</span> <span class="n">builder</span>
</pre></div>

  </div>
</div>


      <div class="class">
          <h3>Ancestors (in MRO)</h3>
          <ul class="class_list">
          <li><a href="#tensorbuilder.core.BuilderTreeBase">BuilderTreeBase</a></li>
          <li>__builtin__.object</li>
          </ul>
          <h3>Methods</h3>
            
  <div class="item">
    <div class="name def" id="tensorbuilder.core.BuilderTreeBase.__init__">
    <p>def <span class="ident">__init__</span>(</p><p>self, builder_iterable)</p>
    </div>
    

    
  
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-tensorbuilder.core.BuilderTreeBase.__init__', this);">Show source &equiv;</a></p>
  <div id="source-tensorbuilder.core.BuilderTreeBase.__init__" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">builder_iterable</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">BuilderTreeBase</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_branches</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">builder_iterable</span><span class="p">)</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    An iterable that can contain elements that are of type `tensorbuilder.core.builders.Builder` or `tensorbuilder.core.builders.BuilderTree`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="tensorbuilder.core.BuilderTreeBase.Builder">
    <p>def <span class="ident">Builder</span>(</p><p>self, tensor)</p>
    </div>
    

    
  
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-tensorbuilder.core.BuilderTreeBase.Builder', this);">Show source &equiv;</a></p>
  <div id="source-tensorbuilder.core.BuilderTreeBase.Builder" class="source">
    <div class="codehilite"><pre><span></span><span class="nd">@abstractmethod</span>
<span class="k">def</span> <span class="nf">Builder</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensor</span><span class="p">):</span>
    <span class="k">pass</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="tensorbuilder.core.BuilderTreeBase.builders">
    <p>def <span class="ident">builders</span>(</p><p>self)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns a flattened list <code>tensorbuilder.core.builders.Builder</code>s contained by this tree. The whole result is flattened in case of sub-elements are also <code>tensorbuilder.core.builders.BuilderTree</code>s.</p>
<p><strong>Return</strong></p>
<ul>
<li><code>list( tensorbuilder.core.builders.Builder )</code></li>
</ul>
<p><strong> Examples </strong></p>
<p>This examples creates a network to that solves the XOR problem using sigmoid units</p>
<div class="codehilite"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>
<span class="kn">from</span> <span class="nn">tensorbuilder</span> <span class="kn">import</span> <span class="n">tb</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>


<span class="c1">#Network</span>
<span class="p">[</span><span class="n">activation_builder</span><span class="p">,</span> <span class="n">trainer_builder</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">tb</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="o">.</span><span class="n">sigmoid_layer</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
    <span class="o">.</span><span class="n">linear_layer</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

    <span class="o">.</span><span class="n">branch</span><span class="p">(</span><span class="k">lambda</span> <span class="n">logit</span><span class="p">:</span>
    <span class="p">[</span>
        <span class="n">logit</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">()</span> <span class="c1"># activation</span>
    <span class="p">,</span>
        <span class="n">logit</span>
        <span class="o">.</span><span class="n">sigmoid_cross_entropy_with_logits</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="c1"># loss</span>
        <span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdamOptimizer</span><span class="p">(</span><span class="mf">0.01</span><span class="p">)</span><span class="o">.</span><span class="n">minimize</span><span class="p">)</span> <span class="c1"># trainer</span>
    <span class="p">])</span>
    <span class="o">.</span><span class="n">builders</span><span class="p">()</span>
<span class="p">)</span>
</pre></div>


<p>Same example using the DSL</p>
<div class="codehilite"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>
<span class="kn">from</span> <span class="nn">tensorbuilder</span> <span class="kn">import</span> <span class="n">tb</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>


<span class="c1">#Network</span>
<span class="p">[</span><span class="n">activation_builder</span><span class="p">,</span> <span class="n">trainer_builder</span><span class="p">]</span> <span class="o">=</span> <span class="n">tb</span><span class="o">.</span><span class="n">pipe</span><span class="p">(</span>
    <span class="n">x</span><span class="p">,</span>
    <span class="n">tb</span><span class="o">.</span><span class="n">sigmoid_layer</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
    <span class="o">.</span><span class="n">linear_layer</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
    <span class="p">[</span>
        <span class="n">tb</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">()</span> <span class="c1"># activation</span>
    <span class="p">,</span>
        <span class="n">tb</span>
        <span class="o">.</span><span class="n">sigmoid_cross_entropy_with_logits</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="c1"># loss</span>
        <span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdamOptimizer</span><span class="p">(</span><span class="mf">0.01</span><span class="p">)</span><span class="o">.</span><span class="n">minimize</span><span class="p">)</span> <span class="c1"># trainer</span>
    <span class="p">],</span>
    <span class="n">tb</span><span class="o">.</span><span class="n">builders</span><span class="p">()</span>
<span class="p">)</span>
</pre></div></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-tensorbuilder.core.BuilderTreeBase.builders', this);">Show source &equiv;</a></p>
  <div id="source-tensorbuilder.core.BuilderTreeBase.builders" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">builders</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a flattened list `tensorbuilder.core.builders.Builder`s contained by this tree. The whole result is flattened in case of sub-elements are also `tensorbuilder.core.builders.BuilderTree`s.</span>
<span class="sd">    **Return**</span>
<span class="sd">    * `list( tensorbuilder.core.builders.Builder )`</span>
<span class="sd">    ** Examples **</span>
<span class="sd">    This examples creates a network to that solves the XOR problem using sigmoid units</span>
<span class="sd">        import tensorflow as tf</span>
<span class="sd">        from tensorbuilder import tb</span>
<span class="sd">        x = tf.placeholder(tf.float32, shape=[None, 2])</span>
<span class="sd">        y = tf.placeholder(tf.float32, shape=[None, 1])</span>
<span class="sd">        #Network</span>
<span class="sd">        [activation_builder, trainer_builder] = (</span>
<span class="sd">            tb.build(x)</span>
<span class="sd">            .sigmoid_layer(2)</span>
<span class="sd">            .linear_layer(1)</span>
<span class="sd">            .branch(lambda logit:</span>
<span class="sd">            [</span>
<span class="sd">                logit.sigmoid() # activation</span>
<span class="sd">            ,</span>
<span class="sd">                logit</span>
<span class="sd">                .sigmoid_cross_entropy_with_logits(y) # loss</span>
<span class="sd">                .map(tf.train.AdamOptimizer(0.01).minimize) # trainer</span>
<span class="sd">            ])</span>
<span class="sd">            .builders()</span>
<span class="sd">        )</span>
<span class="sd">    Same example using the DSL</span>
<span class="sd">        import tensorflow as tf</span>
<span class="sd">        from tensorbuilder import tb</span>
<span class="sd">        x = tf.placeholder(tf.float32, shape=[None, 2])</span>
<span class="sd">        y = tf.placeholder(tf.float32, shape=[None, 1])</span>
<span class="sd">        #Network</span>
<span class="sd">        [activation_builder, trainer_builder] = tb.pipe(</span>
<span class="sd">            x,</span>
<span class="sd">            tb.sigmoid_layer(2)</span>
<span class="sd">            .linear_layer(1),</span>
<span class="sd">            [</span>
<span class="sd">                tb.sigmoid() # activation</span>
<span class="sd">            ,</span>
<span class="sd">                tb</span>
<span class="sd">                .sigmoid_cross_entropy_with_logits(y) # loss</span>
<span class="sd">                .map(tf.train.AdamOptimizer(0.01).minimize) # trainer</span>
<span class="sd">            ],</span>
<span class="sd">            tb.builders()</span>
<span class="sd">        )</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">[</span> <span class="n">builder</span> <span class="k">for</span> <span class="n">builder</span> <span class="ow">in</span> <span class="bp">self</span> <span class="p">]</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="tensorbuilder.core.BuilderTreeBase.copy">
    <p>def <span class="ident">copy</span>(</p><p>self)</p>
    </div>
    

    
  
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-tensorbuilder.core.BuilderTreeBase.copy', this);">Show source &equiv;</a></p>
  <div id="source-tensorbuilder.core.BuilderTreeBase.copy" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">copy</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_unit</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_branches</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="tensorbuilder.core.BuilderTreeBase.extract">
    <p>def <span class="ident">extract</span>(</p><p>tree, fn, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p><code>@immutable</code></p>
<p>Expects a function <strong>fn</strong> with type <code>list( Tensor ) -&gt; Tensor</code> and applies this function to <code>tensorbuilder.core.builders.BuilderTree.tensors</code>, the resulting Tensor is wrapped in Builder. This function</p>
<p><strong>Parameters</strong></p>
<ul>
<li><code>fn</code>: a function of type <code>list( Tensor ) -&gt; Tensor</code>.</li>
<li>All additional *args and **kwargs are forwarded to <code>fn</code></li>
</ul>
<p><strong>Return</strong></p>
<ul>
<li><code>tensorbuilder.core.builders.Builder</code></li>
</ul>
<p><strong> Example </strong></p>
<p>Lets redu the example in <code>tensorbuilder.core.builders.BuilderTree.map_each</code> using <code>extract</code></p>
<div class="codehilite"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>
<span class="kn">from</span> <span class="nn">tensorbuilder</span> <span class="kn">import</span> <span class="n">tb</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">10</span><span class="p">])</span>

<span class="n">h</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">tb</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="o">.</span><span class="n">branch</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="p">[</span>
        <span class="n">x</span><span class="o">.</span><span class="n">relu_layer</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span>
    <span class="p">,</span>
        <span class="n">x</span><span class="o">.</span><span class="n">sigmoid_layer</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span>
    <span class="p">,</span>
        <span class="n">x</span><span class="o">.</span><span class="n">tanh_layer</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span>
    <span class="p">])</span>
    <span class="o">.</span><span class="n">map_each</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">fully_connected</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">activation_fn</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
    <span class="o">.</span><span class="n">extract</span><span class="p">(</span><span class="k">lambda</span> <span class="n">tensors</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">add_n</span><span class="p">(</span><span class="n">tensors</span><span class="p">))</span> <span class="c1">#or just .extract(tf.add_n)</span>
    <span class="o">.</span><span class="n">softmax</span><span class="p">()</span>
    <span class="o">.</span><span class="n">tensor</span><span class="p">()</span>
<span class="p">)</span>
</pre></div>


<p>Same example using the DSL</p>
<div class="codehilite"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>
<span class="kn">from</span> <span class="nn">tensorbuilder</span> <span class="kn">import</span> <span class="n">tb</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">10</span><span class="p">])</span>

<span class="n">h</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">x</span><span class="p">,</span>
    <span class="p">[</span>
        <span class="n">tb</span><span class="o">.</span><span class="n">relu_layer</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span>
    <span class="p">,</span>
        <span class="n">tb</span><span class="o">.</span><span class="n">sigmoid_layer</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span>
    <span class="p">,</span>
        <span class="n">tb</span><span class="o">.</span><span class="n">tanh_layer</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span>
    <span class="p">],</span>
    <span class="n">tb</span><span class="o">.</span><span class="n">map_each</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">fully_connected</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">activation_fn</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
    <span class="o">.</span><span class="n">extract</span><span class="p">(</span><span class="k">lambda</span> <span class="n">tensors</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">add_n</span><span class="p">(</span><span class="n">tensors</span><span class="p">))</span> <span class="c1">#or just .extract(tf.add_n)</span>
    <span class="o">.</span><span class="n">softmax</span><span class="p">()</span>
    <span class="o">.</span><span class="n">tensor</span><span class="p">()</span>
<span class="p">)</span>
</pre></div></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-tensorbuilder.core.BuilderTreeBase.extract', this);">Show source &equiv;</a></p>
  <div id="source-tensorbuilder.core.BuilderTreeBase.extract" class="source">
    <div class="codehilite"><pre><span></span><span class="nd">@immutable</span>
<span class="k">def</span> <span class="nf">extract</span><span class="p">(</span><span class="n">tree</span><span class="p">,</span> <span class="n">fn</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    `@immutable`</span>
<span class="sd">    Expects a function **fn** with type `list( Tensor ) -&gt; Tensor` and applies this function to `tensorbuilder.core.builders.BuilderTree.tensors`, the resulting Tensor is wrapped in Builder. This function</span>
<span class="sd">    **Parameters**</span>
<span class="sd">    * `fn`: a function of type `list( Tensor ) -&gt; Tensor`.</span>
<span class="sd">    * All additional \*args and \*\*kwargs are forwarded to `fn`</span>
<span class="sd">    **Return**</span>
<span class="sd">    * `tensorbuilder.core.builders.Builder`</span>
<span class="sd">    ** Example **</span>
<span class="sd">    Lets redu the example in `tensorbuilder.core.builders.BuilderTree.map_each` using `extract`</span>
<span class="sd">        import tensorflow as tf</span>
<span class="sd">        from tensorbuilder import tb</span>
<span class="sd">        x = placeholder(tf.float32, shape=[None, 10])</span>
<span class="sd">        h = (</span>
<span class="sd">            tb.build(x)</span>
<span class="sd">            .branch(lambda x: [</span>
<span class="sd">                x.relu_layer(20)</span>
<span class="sd">            ,</span>
<span class="sd">                x.sigmoid_layer(20)</span>
<span class="sd">            ,</span>
<span class="sd">                x.tanh_layer(20)</span>
<span class="sd">            ])</span>
<span class="sd">            .map_each(tf.contrib.layers.fully_connected, 5, activation_fn=None)</span>
<span class="sd">            .extract(lambda tensors: tf.add_n(tensors)) #or just .extract(tf.add_n)</span>
<span class="sd">            .softmax()</span>
<span class="sd">            .tensor()</span>
<span class="sd">        )</span>
<span class="sd">    Same example using the DSL</span>
<span class="sd">        import tensorflow as tf</span>
<span class="sd">        from tensorbuilder import tb</span>
<span class="sd">        x = placeholder(tf.float32, shape=[None, 10])</span>
<span class="sd">        h = (</span>
<span class="sd">            x,</span>
<span class="sd">            [</span>
<span class="sd">                tb.relu_layer(20)</span>
<span class="sd">            ,</span>
<span class="sd">                tb.sigmoid_layer(20)</span>
<span class="sd">            ,</span>
<span class="sd">                tb.tanh_layer(20)</span>
<span class="sd">            ],</span>
<span class="sd">            tb.map_each(tf.contrib.layers.fully_connected, 5, activation_fn=None)</span>
<span class="sd">            .extract(lambda tensors: tf.add_n(tensors)) #or just .extract(tf.add_n)</span>
<span class="sd">            .softmax()</span>
<span class="sd">            .tensor()</span>
<span class="sd">        )</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">tensor</span> <span class="o">=</span> <span class="n">fn</span><span class="p">(</span><span class="n">tree</span><span class="o">.</span><span class="n">tensors</span><span class="p">(),</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tree</span><span class="o">.</span><span class="n">Builder</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="tensorbuilder.core.BuilderTreeBase.map_each">
    <p>def <span class="ident">map_each</span>(</p><p>tree, fn, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p><code>@immutable</code></p>
<p>Expects a function <strong>fn</strong> with type <code>Tensor -&gt; Tensor</code> and applies this function to all leaf Tensors separately, resulting in a new BuilderTree.</p>
<p><strong>Parameters</strong></p>
<ul>
<li><code>fn</code>: a function of type <code>Tensor -&gt; Tensor</code>.</li>
<li>All additional *args and **kwargs are forwarded to <code>fn</code></li>
</ul>
<p><strong>Return</strong></p>
<ul>
<li><code>tensorbuilder.core.builders.BuilderTree</code></li>
</ul>
<p><strong> Example </strong></p>
<p>Lets redu the example in <code>tensorbuilder.core.builders.BuilderTree.reduce</code> using <code>map_each</code> to reduce some code</p>
<div class="codehilite"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>
<span class="kn">from</span> <span class="nn">tensorbuilder</span> <span class="kn">import</span> <span class="n">tb</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">10</span><span class="p">])</span>

<span class="n">h</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">tb</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="o">.</span><span class="n">branch</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="p">[</span>
        <span class="n">x</span><span class="o">.</span><span class="n">relu_layer</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span>
    <span class="p">,</span>
        <span class="n">x</span><span class="o">.</span><span class="n">sigmoid_layer</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span>
    <span class="p">,</span>
        <span class="n">x</span><span class="o">.</span><span class="n">tanh_layer</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span>
    <span class="p">])</span>
    <span class="o">.</span><span class="n">map_each</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">fully_connected</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">activation_fn</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
    <span class="o">.</span><span class="n">reduce</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">add</span><span class="p">)</span>
    <span class="o">.</span><span class="n">softmax</span><span class="p">()</span>
    <span class="o">.</span><span class="n">tensor</span><span class="p">()</span>
<span class="p">)</span>
</pre></div>


<p>Remember that this</p>
<div class="codehilite"><pre><span></span>.map_each(tf.contrib.layers.fully_connected, 5, activation_fn=None)
.reduce(tf.add)
.softmax()
</pre></div>


<p>is equivalent to just</p>
<div class="codehilite"><pre><span></span>.softmax_layer(5)
</pre></div>


<p>for <code>BuilderTree</code>s. Same example using the DSL</p>
<div class="codehilite"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>
<span class="kn">from</span> <span class="nn">tensorbuilder</span> <span class="kn">import</span> <span class="n">tb</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">10</span><span class="p">])</span>

<span class="n">h</span> <span class="o">=</span> <span class="n">tb</span><span class="o">.</span><span class="n">pipe</span><span class="p">(</span>
    <span class="n">x</span><span class="p">,</span>
    <span class="p">[</span>
        <span class="n">x</span><span class="o">.</span><span class="n">relu_layer</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span>
    <span class="p">,</span>
        <span class="n">x</span><span class="o">.</span><span class="n">sigmoid_layer</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span>
    <span class="p">,</span>
        <span class="n">x</span><span class="o">.</span><span class="n">tanh_layer</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span>
    <span class="p">],</span>
    <span class="n">tb</span><span class="o">.</span><span class="n">map_each</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">fully_connected</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">activation_fn</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
    <span class="o">.</span><span class="n">reduce</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">add</span><span class="p">)</span>
    <span class="o">.</span><span class="n">softmax</span><span class="p">()</span>
    <span class="o">.</span><span class="n">tensor</span><span class="p">()</span>
<span class="p">)</span>
</pre></div></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-tensorbuilder.core.BuilderTreeBase.map_each', this);">Show source &equiv;</a></p>
  <div id="source-tensorbuilder.core.BuilderTreeBase.map_each" class="source">
    <div class="codehilite"><pre><span></span><span class="nd">@immutable</span>
<span class="k">def</span> <span class="nf">map_each</span><span class="p">(</span><span class="n">tree</span><span class="p">,</span> <span class="n">fn</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    `@immutable`</span>
<span class="sd">    Expects a function **fn** with type `Tensor -&gt; Tensor` and applies this function to all leaf Tensors separately, resulting in a new BuilderTree.</span>
<span class="sd">    **Parameters**</span>
<span class="sd">    * `fn`: a function of type `Tensor -&gt; Tensor`.</span>
<span class="sd">    * All additional \*args and \*\*kwargs are forwarded to `fn`</span>
<span class="sd">    **Return**</span>
<span class="sd">    * `tensorbuilder.core.builders.BuilderTree`</span>
<span class="sd">    ** Example **</span>
<span class="sd">    Lets redu the example in `tensorbuilder.core.builders.BuilderTree.reduce` using `map_each` to reduce some code</span>
<span class="sd">        import tensorflow as tf</span>
<span class="sd">        from tensorbuilder import tb</span>
<span class="sd">        x = placeholder(tf.float32, shape=[None, 10])</span>
<span class="sd">        h = (</span>
<span class="sd">            tb.build(x)</span>
<span class="sd">            .branch(lambda x: [</span>
<span class="sd">                x.relu_layer(20)</span>
<span class="sd">            ,</span>
<span class="sd">                x.sigmoid_layer(20)</span>
<span class="sd">            ,</span>
<span class="sd">                x.tanh_layer(20)</span>
<span class="sd">            ])</span>
<span class="sd">            .map_each(tf.contrib.layers.fully_connected, 5, activation_fn=None)</span>
<span class="sd">            .reduce(tf.add)</span>
<span class="sd">            .softmax()</span>
<span class="sd">            .tensor()</span>
<span class="sd">        )</span>
<span class="sd">    Remember that this</span>
<span class="sd">        .map_each(tf.contrib.layers.fully_connected, 5, activation_fn=None)</span>
<span class="sd">        .reduce(tf.add)</span>
<span class="sd">        .softmax()</span>
<span class="sd">    is equivalent to just</span>
<span class="sd">        .softmax_layer(5)</span>
<span class="sd">    for `BuilderTree`s. Same example using the DSL</span>
<span class="sd">        import tensorflow as tf</span>
<span class="sd">        from tensorbuilder import tb</span>
<span class="sd">        x = placeholder(tf.float32, shape=[None, 10])</span>
<span class="sd">        h = tb.pipe(</span>
<span class="sd">            x,</span>
<span class="sd">            [</span>
<span class="sd">                x.relu_layer(20)</span>
<span class="sd">            ,</span>
<span class="sd">                x.sigmoid_layer(20)</span>
<span class="sd">            ,</span>
<span class="sd">                x.tanh_layer(20)</span>
<span class="sd">            ],</span>
<span class="sd">            tb.map_each(tf.contrib.layers.fully_connected, 5, activation_fn=None)</span>
<span class="sd">            .reduce(tf.add)</span>
<span class="sd">            .softmax()</span>
<span class="sd">            .tensor()</span>
<span class="sd">        )</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">branches</span> <span class="o">=</span> <span class="p">[</span> <span class="n">builder</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="k">for</span> <span class="n">builder</span> <span class="ow">in</span> <span class="n">tree</span> <span class="p">]</span>
    <span class="k">return</span> <span class="n">tree</span><span class="o">.</span><span class="n">_unit</span><span class="p">(</span><span class="n">branches</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="tensorbuilder.core.BuilderTreeBase.reduce">
    <p>def <span class="ident">reduce</span>(</p><p>tree, fn, initializer=None)</p>
    </div>
    

    
  
    <div class="desc"><p><code>@immutable</code></p>
<p>Expects a function <strong>fn</strong> with type <code>(Tensor, Tensor) -&gt; Tensor</code> and optionally an <code>initializer</code> and applies python <a href="https://docs.python.org/2/library/functions.html#reduce">reduce</a> function to <code>tensorbuilder.core.builders.BuilderTree.tensors</code> using these arguments; the resulting Tensor is the wrapped inside a Builder.</p>
<p><strong>Parameters</strong></p>
<ul>
<li><code>fn</code>: a function of type <code>(Tensor, Tensor) -&gt; Tensor</code>.</li>
<li><code>initializer</code>: an optional Tensor as initial element of the folding operation (default: <code>None</code>)s</li>
</ul>
<p><strong>Return</strong></p>
<ul>
<li><code>tensorbuilder.core.builders.Builder</code></li>
</ul>
<p><strong> Example </strong></p>
<p>Lets reduce the example on <code>tensorbuilder.core.builders.Builder.branch</code> this time doing the reduction ourselves instead of relying on the <code>*_layer</code> of <code>tensorbuilder.core.builders.BuilderTree</code> that do this for us</p>
<div class="codehilite"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>
<span class="kn">from</span> <span class="nn">tensorbuilder</span> <span class="kn">import</span> <span class="n">tb</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">10</span><span class="p">])</span>

<span class="n">h</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">tb</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="o">.</span><span class="n">branch</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="p">[</span>
        <span class="n">x</span><span class="o">.</span><span class="n">relu_layer</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span>
        <span class="o">.</span><span class="n">linear_layer</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
    <span class="p">,</span>
        <span class="n">x</span><span class="o">.</span><span class="n">sigmoid_layer</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span>
        <span class="o">.</span><span class="n">linear_layer</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
    <span class="p">,</span>
        <span class="n">x</span><span class="o">.</span><span class="n">tanh_layer</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span>
        <span class="o">.</span><span class="n">linear_layer</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
    <span class="p">])</span>
    <span class="o">.</span><span class="n">reduce</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">add</span><span class="p">)</span>
    <span class="o">.</span><span class="n">softmax</span><span class="p">()</span>
    <span class="o">.</span><span class="n">tensor</span><span class="p">()</span>
<span class="p">)</span>
</pre></div>


<p>Same example using the DSL</p>
<div class="codehilite"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>
<span class="kn">from</span> <span class="nn">tensorbuilder</span> <span class="kn">import</span> <span class="n">tb</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">10</span><span class="p">])</span>

<span class="n">h</span> <span class="o">=</span> <span class="n">tb</span><span class="o">.</span><span class="n">pipe</span><span class="p">(</span>
    <span class="n">x</span><span class="p">,</span>
    <span class="p">[</span>
        <span class="n">tb</span><span class="o">.</span><span class="n">relu_layer</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span>
        <span class="o">.</span><span class="n">linear_layer</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
    <span class="p">,</span>
        <span class="n">tb</span><span class="o">.</span><span class="n">sigmoid_layer</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span>
        <span class="o">.</span><span class="n">linear_layer</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
    <span class="p">,</span>
        <span class="n">tb</span><span class="o">.</span><span class="n">tanh_layer</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span>
        <span class="o">.</span><span class="n">linear_layer</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
    <span class="p">],</span>
    <span class="n">tb</span><span class="o">.</span><span class="n">reduce</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">add</span><span class="p">)</span>
    <span class="o">.</span><span class="n">softmax</span><span class="p">()</span>
    <span class="o">.</span><span class="n">tensor</span><span class="p">()</span>
<span class="p">)</span>
</pre></div></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-tensorbuilder.core.BuilderTreeBase.reduce', this);">Show source &equiv;</a></p>
  <div id="source-tensorbuilder.core.BuilderTreeBase.reduce" class="source">
    <div class="codehilite"><pre><span></span><span class="nd">@immutable</span>
<span class="k">def</span> <span class="nf">reduce</span><span class="p">(</span><span class="n">tree</span><span class="p">,</span> <span class="n">fn</span><span class="p">,</span> <span class="n">initializer</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    `@immutable`</span>
<span class="sd">    Expects a function **fn** with type `(Tensor, Tensor) -&gt; Tensor` and optionally an `initializer` and applies python [reduce](https://docs.python.org/2/library/functions.html#reduce) function to `tensorbuilder.core.builders.BuilderTree.tensors` using these arguments; the resulting Tensor is the wrapped inside a Builder.</span>
<span class="sd">    **Parameters**</span>
<span class="sd">    * `fn`: a function of type `(Tensor, Tensor) -&gt; Tensor`.</span>
<span class="sd">    * `initializer`: an optional Tensor as initial element of the folding operation (default: `None`)s</span>
<span class="sd">    **Return**</span>
<span class="sd">    * `tensorbuilder.core.builders.Builder`</span>
<span class="sd">    ** Example **</span>
<span class="sd">    Lets reduce the example on `tensorbuilder.core.builders.Builder.branch` this time doing the reduction ourselves instead of relying on the `*_layer` of `tensorbuilder.core.builders.BuilderTree` that do this for us</span>
<span class="sd">        import tensorflow as tf</span>
<span class="sd">        from tensorbuilder import tb</span>
<span class="sd">        x = placeholder(tf.float32, shape=[None, 10])</span>
<span class="sd">        h = (</span>
<span class="sd">            tb.build(x)</span>
<span class="sd">            .branch(lambda x: [</span>
<span class="sd">                x.relu_layer(20)</span>
<span class="sd">                .linear_layer(5)</span>
<span class="sd">            ,</span>
<span class="sd">                x.sigmoid_layer(20)</span>
<span class="sd">                .linear_layer(5)</span>
<span class="sd">            ,</span>
<span class="sd">                x.tanh_layer(20)</span>
<span class="sd">                .linear_layer(5)</span>
<span class="sd">            ])</span>
<span class="sd">            .reduce(tf.add)</span>
<span class="sd">            .softmax()</span>
<span class="sd">            .tensor()</span>
<span class="sd">        )</span>
<span class="sd">    Same example using the DSL</span>
<span class="sd">        import tensorflow as tf</span>
<span class="sd">        from tensorbuilder import tb</span>
<span class="sd">        x = placeholder(tf.float32, shape=[None, 10])</span>
<span class="sd">        h = tb.pipe(</span>
<span class="sd">            x,</span>
<span class="sd">            [</span>
<span class="sd">                tb.relu_layer(20)</span>
<span class="sd">                .linear_layer(5)</span>
<span class="sd">            ,</span>
<span class="sd">                tb.sigmoid_layer(20)</span>
<span class="sd">                .linear_layer(5)</span>
<span class="sd">            ,</span>
<span class="sd">                tb.tanh_layer(20)</span>
<span class="sd">                .linear_layer(5)</span>
<span class="sd">            ],</span>
<span class="sd">            tb.reduce(tf.add)</span>
<span class="sd">            .softmax()</span>
<span class="sd">            .tensor()</span>
<span class="sd">        )</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">initializer</span> <span class="o">!=</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">tensor</span> <span class="o">=</span> <span class="n">functools</span><span class="o">.</span><span class="n">reduce</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="n">tree</span><span class="o">.</span><span class="n">tensors</span><span class="p">(),</span> <span class="n">initializer</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">tensor</span> <span class="o">=</span> <span class="n">functools</span><span class="o">.</span><span class="n">reduce</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="n">tree</span><span class="o">.</span><span class="n">tensors</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">tree</span><span class="o">.</span><span class="n">Builder</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="tensorbuilder.core.BuilderTreeBase.register_method">
    <p>def <span class="ident">register_method</span>(</p><p>cls, fn, library_path, alias=None, doc=None)</p>
    </div>
    

    
  
    <div class="desc"><p>This method enables you to register any function <code>fn</code> that takes a BuilderTree as its first argument as a method of the Builder class.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>fn</code>: a function that atleast takes a BuilderTree as its first argument.</li>
<li><code>library_path</code>: the route of the librar from which this function was taken, used for documentation purposes.</li>
<li><code>alias</code>: allows you to specify the name of the method, it will take the name of the function if its <code>None</code>.</li>
<li><code>doc</code>: the documentation for the method, if <code>None</code> a predefied documentation will be generated based on the documentation of <code>fn</code>.</li>
</ul>
<p><strong>Return</strong></p>
<p><code>None</code></p>
<p><strong>Examples</strong></p>
<p>In this example we will create the method <code>fully_connected</code> for the BuilderTree class</p>
<div class="codehilite"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>
<span class="kn">from</span> <span class="nn">tensorbuilder</span> <span class="kn">import</span> <span class="n">tb</span>

<span class="k">def</span> <span class="nf">_tree_fully_connected</span><span class="p">(</span><span class="n">tree</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="n">activation_fn</span> <span class="o">=</span> <span class="bp">None</span>

    <span class="k">if</span> <span class="s2">&quot;activation_fn&quot;</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">:</span>
        <span class="n">activation_fn</span> <span class="o">=</span> <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;activation_fn&quot;</span><span class="p">]</span>
        <span class="k">del</span> <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;activation_fn&quot;</span><span class="p">]</span>

    <span class="n">builder</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">tree</span><span class="o">.</span><span class="n">map_each</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">fully_connected</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="o">.</span><span class="n">reduce</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">add</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="n">activation_fn</span><span class="p">:</span>
        <span class="n">builder</span> <span class="o">=</span> <span class="n">builder</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">activation_fn</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">builder</span>

<span class="n">tb</span><span class="o">.</span><span class="n">BuilderTree</span><span class="o">.</span><span class="n">register_method</span><span class="p">(</span><span class="n">_tree_fully_connected</span><span class="p">,</span> <span class="s2">&quot;tensorbuilder.patches.tensorflow.fully_connected&quot;</span><span class="p">,</span> <span class="n">alias</span><span class="o">=</span><span class="s2">&quot;fully_connected&quot;</span><span class="p">)</span>
</pre></div></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-tensorbuilder.core.BuilderTreeBase.register_method', this);">Show source &equiv;</a></p>
  <div id="source-tensorbuilder.core.BuilderTreeBase.register_method" class="source">
    <div class="codehilite"><pre><span></span><span class="nd">@classmethod</span>
<span class="k">def</span> <span class="nf">register_method</span><span class="p">(</span><span class="n">cls</span><span class="p">,</span> <span class="n">fn</span><span class="p">,</span> <span class="n">library_path</span><span class="p">,</span> <span class="n">alias</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">doc</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This method enables you to register any function `fn` that takes a BuilderTree as its first argument as a method of the Builder class.</span>
<span class="sd">    **Arguments**</span>
<span class="sd">    * `fn`: a function that atleast takes a BuilderTree as its first argument.</span>
<span class="sd">    * `library_path`: the route of the librar from which this function was taken, used for documentation purposes.</span>
<span class="sd">    * `alias`: allows you to specify the name of the method, it will take the name of the function if its `None`.</span>
<span class="sd">    * `doc`: the documentation for the method, if `None` a predefied documentation will be generated based on the documentation of `fn`.</span>
<span class="sd">    **Return**</span>
<span class="sd">    `None`</span>
<span class="sd">    **Examples**</span>
<span class="sd">    In this example we will create the method `fully_connected` for the BuilderTree class</span>
<span class="sd">        import tensorflow as tf</span>
<span class="sd">        from tensorbuilder import tb</span>
<span class="sd">        def _tree_fully_connected(tree, size, *args, **kwargs):</span>
<span class="sd">            activation_fn = None</span>
<span class="sd">            if &quot;activation_fn&quot; in kwargs:</span>
<span class="sd">                activation_fn = kwargs[&quot;activation_fn&quot;]</span>
<span class="sd">                del kwargs[&quot;activation_fn&quot;]</span>
<span class="sd">            builder = (</span>
<span class="sd">                tree.map_each(tf.contrib.layers.fully_connected, size, *args, **kwargs)</span>
<span class="sd">                .reduce(tf.add)</span>
<span class="sd">            )</span>
<span class="sd">            if activation_fn:</span>
<span class="sd">                builder = builder.map(activation_fn)</span>
<span class="sd">            return builder</span>
<span class="sd">        tb.BuilderTree.register_method(_tree_fully_connected, &quot;tensorbuilder.patches.tensorflow.fully_connected&quot;, alias=&quot;fully_connected&quot;)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">fn_signature</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">get_method_sig</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span>
 	<span class="n">fn_docs</span> <span class="o">=</span> <span class="n">inspect</span><span class="o">.</span><span class="n">getdoc</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span>
    <span class="n">original_name</span> <span class="o">=</span> <span class="n">fn</span><span class="o">.</span><span class="n">__name__</span>
    <span class="n">name</span> <span class="o">=</span> <span class="n">alias</span> <span class="k">if</span> <span class="n">alias</span> <span class="k">else</span> <span class="n">original_name</span>
    <span class="n">fn</span><span class="o">.</span><span class="n">__name__</span> <span class="o">=</span> <span class="n">name</span>
    <span class="n">fn</span><span class="o">.</span><span class="n">__doc__</span> <span class="o">=</span> <span class="n">doc</span> <span class="k">if</span> <span class="n">doc</span> <span class="k">else</span> <span class="n">_tree_register_method_docs</span><span class="p">(</span><span class="n">original_name</span><span class="p">,</span> <span class="n">library_path</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">fn_signature</span><span class="p">,</span> <span class="n">fn_docs</span><span class="p">)</span>
    <span class="nb">setattr</span><span class="p">(</span><span class="n">cls</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">fn</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="tensorbuilder.core.BuilderTreeBase.register_reduce_method">
    <p>def <span class="ident">register_reduce_method</span>(</p><p>cls, fn, library_path, alias=None, doc=None)</p>
    </div>
    

    
  
    <div class="desc"><p>This method enables you to register a function <code>fn</code> of type <code>(Tensor, Tensor) -&gt; Tensor</code> as a method of the Builder class.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>fn</code>: a function of type <code>(Tensor, Tensor) -&gt; Tensor</code></li>
<li><code>library_path</code>: the route of the librar from which this function was taken, used for documentation purposes.</li>
<li><code>alias</code>: allows you to specify the name of the method, it will take the name of the function if its <code>None</code>.</li>
<li><code>doc</code>: the documentation for the method, if <code>None</code> a predefied documentation will be generated based on the documentation of <code>fn</code>.</li>
</ul>
<p><strong>Return</strong></p>
<p><code>None</code></p>
<p><strong>Examples</strong></p>
<p>In this example we will create the method <code>reduce_add</code> for the BuilderTree class</p>
<div class="codehilite"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>
<span class="kn">from</span> <span class="nn">tensorbuilder</span> <span class="kn">import</span> <span class="n">tb</span>

<span class="n">tb</span><span class="o">.</span><span class="n">BuilderTree</span><span class="o">.</span><span class="n">register_reduce_method</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">add</span><span class="p">,</span> <span class="s2">&quot;tf&quot;</span><span class="p">,</span> <span class="n">alias</span><span class="o">=</span><span class="s2">&quot;reduce_add&quot;</span><span class="p">)</span>
</pre></div></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-tensorbuilder.core.BuilderTreeBase.register_reduce_method', this);">Show source &equiv;</a></p>
  <div id="source-tensorbuilder.core.BuilderTreeBase.register_reduce_method" class="source">
    <div class="codehilite"><pre><span></span><span class="nd">@classmethod</span>
<span class="k">def</span> <span class="nf">register_reduce_method</span><span class="p">(</span><span class="n">cls</span><span class="p">,</span> <span class="n">fn</span><span class="p">,</span> <span class="n">library_path</span><span class="p">,</span> <span class="n">alias</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">doc</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This method enables you to register a function `fn` of type `(Tensor, Tensor) -&gt; Tensor` as a method of the Builder class.</span>
<span class="sd">    **Arguments**</span>
<span class="sd">    * `fn`: a function of type `(Tensor, Tensor) -&gt; Tensor`</span>
<span class="sd">    * `library_path`: the route of the librar from which this function was taken, used for documentation purposes.</span>
<span class="sd">    * `alias`: allows you to specify the name of the method, it will take the name of the function if its `None`.</span>
<span class="sd">    * `doc`: the documentation for the method, if `None` a predefied documentation will be generated based on the documentation of `fn`.</span>
<span class="sd">    **Return**</span>
<span class="sd">    `None`</span>
<span class="sd">    **Examples**</span>
<span class="sd">    In this example we will create the method `reduce_add` for the BuilderTree class</span>
<span class="sd">        import tensorflow as tf</span>
<span class="sd">        from tensorbuilder import tb</span>
<span class="sd">        tb.BuilderTree.register_reduce_method(tf.add, &quot;tf&quot;, alias=&quot;reduce_add&quot;)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">fn_signature</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">get_method_sig</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span>
 	<span class="n">fn_docs</span> <span class="o">=</span> <span class="n">inspect</span><span class="o">.</span><span class="n">getdoc</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span>
    <span class="n">original_name</span> <span class="o">=</span> <span class="n">fn</span><span class="o">.</span><span class="n">__name__</span>
    <span class="n">name</span> <span class="o">=</span> <span class="n">alias</span> <span class="k">if</span> <span class="n">alias</span> <span class="k">else</span> <span class="n">original_name</span>
    <span class="n">_tree_method</span> <span class="o">=</span> <span class="n">_lift_tree_reduce</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span>
    <span class="n">_tree_method</span><span class="o">.</span><span class="n">__name__</span> <span class="o">=</span> <span class="n">name</span>
    <span class="n">_tree_method</span><span class="o">.</span><span class="n">__doc__</span> <span class="o">=</span> <span class="n">doc</span> <span class="k">if</span> <span class="n">doc</span> <span class="k">else</span> <span class="n">_tree_register_reduce_method_docs</span><span class="p">(</span><span class="n">original_name</span><span class="p">,</span> <span class="n">library_path</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">fn_signature</span><span class="p">,</span> <span class="n">fn_docs</span><span class="p">)</span>
    <span class="nb">setattr</span><span class="p">(</span><span class="n">cls</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">_tree_method</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="tensorbuilder.core.BuilderTreeBase.tensors">
    <p>def <span class="ident">tensors</span>(</p><p>self)</p>
    </div>
    

    
  
    <div class="desc"><p>Same as <code>tensorbuilder.core.builders.BuilderTree.builders</code> but extracts the tensor from each <code>tensorbuilder.core.builders.Builder</code>.</p>
<p><strong>Return</strong></p>
<ul>
<li><code>list( tf.Tensor )</code></li>
</ul>
<p><strong> Example </strong></p>
<p>This examples creates a network to that solves the XOR problem using sigmoid units</p>
<div class="codehilite"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>
<span class="kn">from</span> <span class="nn">tensorbuilder</span> <span class="kn">import</span> <span class="n">tb</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>


<span class="c1">#Network</span>
<span class="p">[</span><span class="n">activation_tensor</span><span class="p">,</span> <span class="n">trainer_tensor</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">tb</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="o">.</span><span class="n">sigmoid_layer</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
    <span class="o">.</span><span class="n">linear_layer</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

    <span class="o">.</span><span class="n">branch</span><span class="p">(</span><span class="k">lambda</span> <span class="n">logit</span><span class="p">:</span>
    <span class="p">[</span>
        <span class="n">logit</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">()</span> <span class="c1"># activation</span>
    <span class="p">,</span>
        <span class="n">logit</span>
        <span class="o">.</span><span class="n">sigmoid_cross_entropy_with_logits</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="c1"># loss</span>
        <span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdamOptimizer</span><span class="p">(</span><span class="mf">0.01</span><span class="p">)</span><span class="o">.</span><span class="n">minimize</span><span class="p">)</span> <span class="c1"># trainer</span>
    <span class="p">])</span>
    <span class="o">.</span><span class="n">tensors</span><span class="p">()</span>
<span class="p">)</span>
</pre></div>


<p>Same example using the DSL</p>
<div class="codehilite"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>
<span class="kn">from</span> <span class="nn">tensorbuilder</span> <span class="kn">import</span> <span class="n">tb</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>


<span class="c1">#Network</span>
<span class="p">[</span><span class="n">activation_tensor</span><span class="p">,</span> <span class="n">trainer_tensor</span><span class="p">]</span> <span class="o">=</span> <span class="n">tb</span><span class="o">.</span><span class="n">pipe</span><span class="p">(</span>
    <span class="n">x</span><span class="p">,</span>
    <span class="n">tb</span><span class="o">.</span><span class="n">sigmoid_layer</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
    <span class="o">.</span><span class="n">linear_layer</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
    <span class="p">[</span>
        <span class="n">tb</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">()</span> <span class="c1"># activation</span>
    <span class="p">,</span>
        <span class="n">tb</span>
        <span class="o">.</span><span class="n">sigmoid_cross_entropy_with_logits</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="c1"># loss</span>
        <span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdamOptimizer</span><span class="p">(</span><span class="mf">0.01</span><span class="p">)</span><span class="o">.</span><span class="n">minimize</span><span class="p">)</span> <span class="c1"># trainer</span>
    <span class="p">],</span>
    <span class="n">tb</span><span class="o">.</span><span class="n">tensors</span><span class="p">()</span>
<span class="p">)</span>
</pre></div></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-tensorbuilder.core.BuilderTreeBase.tensors', this);">Show source &equiv;</a></p>
  <div id="source-tensorbuilder.core.BuilderTreeBase.tensors" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">tensors</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Same as `tensorbuilder.core.builders.BuilderTree.builders` but extracts the tensor from each `tensorbuilder.core.builders.Builder`.</span>
<span class="sd">    **Return**</span>
<span class="sd">    * `list( tf.Tensor )`</span>
<span class="sd">    ** Example **</span>
<span class="sd">    This examples creates a network to that solves the XOR problem using sigmoid units</span>
<span class="sd">        import tensorflow as tf</span>
<span class="sd">        from tensorbuilder import tb</span>
<span class="sd">        x = tf.placeholder(tf.float32, shape=[None, 2])</span>
<span class="sd">        y = tf.placeholder(tf.float32, shape=[None, 1])</span>
<span class="sd">        #Network</span>
<span class="sd">        [activation_tensor, trainer_tensor] = (</span>
<span class="sd">            tb.build(x)</span>
<span class="sd">            .sigmoid_layer(2)</span>
<span class="sd">            .linear_layer(1)</span>
<span class="sd">            .branch(lambda logit:</span>
<span class="sd">            [</span>
<span class="sd">                logit.sigmoid() # activation</span>
<span class="sd">            ,</span>
<span class="sd">                logit</span>
<span class="sd">                .sigmoid_cross_entropy_with_logits(y) # loss</span>
<span class="sd">                .map(tf.train.AdamOptimizer(0.01).minimize) # trainer</span>
<span class="sd">            ])</span>
<span class="sd">            .tensors()</span>
<span class="sd">        )</span>
<span class="sd">    Same example using the DSL</span>
<span class="sd">        import tensorflow as tf</span>
<span class="sd">        from tensorbuilder import tb</span>
<span class="sd">        x = tf.placeholder(tf.float32, shape=[None, 2])</span>
<span class="sd">        y = tf.placeholder(tf.float32, shape=[None, 1])</span>
<span class="sd">        #Network</span>
<span class="sd">        [activation_tensor, trainer_tensor] = tb.pipe(</span>
<span class="sd">            x,</span>
<span class="sd">            tb.sigmoid_layer(2)</span>
<span class="sd">            .linear_layer(1),</span>
<span class="sd">            [</span>
<span class="sd">                tb.sigmoid() # activation</span>
<span class="sd">            ,</span>
<span class="sd">                tb</span>
<span class="sd">                .sigmoid_cross_entropy_with_logits(y) # loss</span>
<span class="sd">                .map(tf.train.AdamOptimizer(0.01).minimize) # trainer</span>
<span class="sd">            ],</span>
<span class="sd">            tb.tensors()</span>
<span class="sd">        )</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">[</span> <span class="n">builder</span><span class="o">.</span><span class="n">_tensor</span> <span class="k">for</span> <span class="n">builder</span> <span class="ow">in</span> <span class="bp">self</span> <span class="p">]</span>
</pre></div>

  </div>
</div>

  </div>
  
      </div>
      </div>

    <h2 class="section-title" id="header-submodules">Sub-modules</h2>
      <div class="item">
      <p class="name"><a href="concrete_classes.m.html">tensorbuilder.core.concrete_classes</a></p>
      
  

      </div>
      <div class="item">
      <p class="name"><a href="utils.m.html">tensorbuilder.core.utils</a></p>
      
  

      </div>
  </section>

    </article>
  <div class="clear"> </div>
  <footer id="footer">
    <p>
      Documentation generated by
      <a href="https://github.com/BurntSushi/pdoc">pdoc 0.3.2</a>
    </p>

    <p>pdoc is in the public domain with the
      <a href="http://unlicense.org">UNLICENSE</a></p>

    <p>Design by <a href="http://nadh.in">Kailash Nadh</a></p>
  </footer>
</div>
</body>
</html>
